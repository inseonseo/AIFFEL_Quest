{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "742c24dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version:1.12.1\n",
      "Cuda version: 11.3\n",
      "transformers version: 4.28.0\n",
      "GPU 사용 가능여부: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "\n",
    "print(\"Torch version:{}\".format(torch.__version__)) # Torch version:1.12.1\n",
    "print(\"Cuda version: {}\".format(torch.version.cuda)) # Cuda version: 11.3\n",
    "print(\"transformers version: {}\".format(transformers.__version__)) # transformers 4.28.0\n",
    "print(\"GPU 사용 가능여부: {}\".format(torch.cuda.is_available()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ddaffd",
   "metadata": {},
   "source": [
    "## 1. Base model and Dataset for RLHF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d80d5dac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "407b9abce82548bca553dfbbaa1c0b6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/2.83M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79405333a80c475a8d8abbfff0d050a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/513M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import pandas as pd\n",
    "import numpy\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_name = \"skt/kogpt2-base-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d59177e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gpt2': 1024,\n",
       " 'gpt2-medium': 1024,\n",
       " 'gpt2-large': 1024,\n",
       " 'gpt2-xl': 1024,\n",
       " 'distilgpt2': 1024}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 우리가 사용할 모델의 토크나이저가 입력받아 처리할 수 있는 최대 토큰 수\n",
    "tokenizer.max_model_input_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbfdb3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kogpt-2 tokenizing\n",
    "input_txt = \"바람도 없는 공중에 수직의 파문을 내이며 고요히 떨어지는 오동잎은 누구의 발자취 입니까.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a649970",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer(input_txt).tokens()\n",
    "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b4ad35c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>kogpt-2_tokens</th>\n",
       "      <td>▁바람</td>\n",
       "      <td>도</td>\n",
       "      <td>▁없는</td>\n",
       "      <td>▁공중에</td>\n",
       "      <td>▁수직</td>\n",
       "      <td>의</td>\n",
       "      <td>▁파</td>\n",
       "      <td>문을</td>\n",
       "      <td>▁내</td>\n",
       "      <td>이며</td>\n",
       "      <td>▁고</td>\n",
       "      <td>요</td>\n",
       "      <td>히</td>\n",
       "      <td>▁떨어지는</td>\n",
       "      <td>▁오동</td>\n",
       "      <td>잎은</td>\n",
       "      <td>▁누</td>\n",
       "      <td>구의</td>\n",
       "      <td>▁발자</td>\n",
       "      <td>취</td>\n",
       "      <td>▁입</td>\n",
       "      <td>니까</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Input_IDs</th>\n",
       "      <td>10891</td>\n",
       "      <td>7235</td>\n",
       "      <td>9712</td>\n",
       "      <td>49207</td>\n",
       "      <td>14438</td>\n",
       "      <td>8143</td>\n",
       "      <td>9203</td>\n",
       "      <td>9941</td>\n",
       "      <td>9094</td>\n",
       "      <td>9639</td>\n",
       "      <td>9065</td>\n",
       "      <td>8084</td>\n",
       "      <td>8811</td>\n",
       "      <td>21215</td>\n",
       "      <td>34769</td>\n",
       "      <td>19985</td>\n",
       "      <td>9669</td>\n",
       "      <td>10139</td>\n",
       "      <td>21626</td>\n",
       "      <td>8408</td>\n",
       "      <td>9241</td>\n",
       "      <td>23775</td>\n",
       "      <td>389</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0     1     2      3      4     5     6     7     8     9   \\\n",
       "kogpt-2_tokens    ▁바람     도   ▁없는   ▁공중에    ▁수직     의    ▁파    문을    ▁내    이며   \n",
       "Input_IDs       10891  7235  9712  49207  14438  8143  9203  9941  9094  9639   \n",
       "\n",
       "                  10    11    12     13     14     15    16     17     18  \\\n",
       "kogpt-2_tokens    ▁고     요     히  ▁떨어지는    ▁오동     잎은    ▁누     구의    ▁발자   \n",
       "Input_IDs       9065  8084  8811  21215  34769  19985  9669  10139  21626   \n",
       "\n",
       "                  19    20     21   22  \n",
       "kogpt-2_tokens     취    ▁입     니까    .  \n",
       "Input_IDs       8408  9241  23775  389  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.max_columns = 40\n",
    "pd.options.display.max_rows = 60\n",
    "df = pd.DataFrame([tokens, input_ids[0]], index=[\"kogpt-2_tokens\", \"Input_IDs\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8c6a106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "바람도 없는 공중에 수직의 파문을 내이며 고요히 떨어지는 오동잎은 누구의 발자취 입니까.'\n",
      "\"그렇다면 그건 무슨 소리요?\"\n",
      "\"그건 무슨 소리요?\"\n",
      "\"그건 무슨 소리요?\"\n",
      "\"그건 무슨 소리요?\"\n",
      "\"그건 무슨 소리요?\"\n",
      "\"그건 무슨 소리요?\"\n",
      "\"그건 무슨 소리요?\"\n",
      "\"그건 무슨 소리요?\"\n",
      "\"그건 무슨 소리요?\"\n",
      "\"그건 무슨 소리요?\"\n",
      "\"그건 무슨 소리요?\"\n",
      "\"그건 무슨 소리요?\"\n",
      "\"그건 무슨 소리\n"
     ]
    }
   ],
   "source": [
    "# decoding performance\n",
    "max_length=128\n",
    "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "output_greedy = model.generate(input_ids, max_length=max_length, do_sample=False)\n",
    "print(tokenizer.decode(output_greedy[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5e6ab1",
   "metadata": {},
   "source": [
    "시퀀스가 반복되어 출력되는군요.\n",
    "그리디 서치 디코딩시 발견되는 전형적인 현상입니다.\n",
    "\n",
    "이번엔 빔 서치 디코딩을 사용하고 n-gram 패널티까지 부과해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "050117d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "바람도 없는 공중에 수직의 파문을 내이며 고요히 떨어지는 오동잎은 누구의 발자취 입니까.'\n",
      "\"그렇지 않습니다.\"\n",
      "\"어떻게 된 일입니까?\"\n",
      "그녀는 고개를 갸웃거렸다.\n",
      "\"아니, 그게 무슨 말씀이신지 모르겠습니다만.\"\n",
      "\"무슨 말씀인지 알 수가 없군요.\"\n",
      "아무런 대답도 하지 않은 채 그녀는 고개를 끄덕였다.\n",
      "\"그래, 알았어.\"\n",
      "그녀의 눈에서 눈물이 주르륵 흘러내렸다.\n",
      "그녀가 다시 입을 열었다.\n",
      "\"정말 죄송합니다, 고마워요, 고맙습니다\"\n",
      "\"\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "output_beam = model.generate(input_ids, max_length=max_length, num_beams=10, no_repeat_ngram_size=2,\n",
    "                             do_sample=False)\n",
    "print(tokenizer.decode(output_beam[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97cb477",
   "metadata": {},
   "source": [
    "- 입력 시퀀스와 별 상관 없어 보이는 긴 문단이 생성됩니다.\n",
    "- 그럼에도 생성된 문단은 제법 맥락을 갖춘 듯 보입니다.\n",
    "- 하지만 문장 간의 정합성이나 일관성은 다소 떨어지는 부분도 관찰됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1326d19a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "바람도 없는 공중에 수직의 파문을 내이며 고요히 떨어지는 오동잎은 누구의 발자취 입니까. 횃불도 없었어요, 고마워요.\"\n",
      "사내가 다급하게 부르짖었다.\n",
      "\"그렇게 하지 않을래요?\"\n",
      "\"그러지 않아도 될 거요.\" 사나이가 외쳤다.\n",
      "사내의 얼굴에서 희열이 치솟았다.\n",
      "\"안 돼요. 당신들 얘기 듣고 나서 말이오.\" 사나이는 또 한 번 몸을 돌려 소리를 질렀다.\n",
      "\"나도 당신이 원하는 대로 살 수 있소.\"\n",
      "나는 고개를 절레절레 흔들었다. 고삐가 풀리는 소리였다.\n",
      "\"지금 당장 그쪽으로\n"
     ]
    }
   ],
   "source": [
    "# sampling 추가\n",
    "output_beam = model.generate(input_ids, max_length=max_length, num_beams=7, no_repeat_ngram_size=2,\n",
    "                             do_sample=True, temperature=2.0, top_k=50)\n",
    "print(tokenizer.decode(output_beam[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5b5c86",
   "metadata": {},
   "source": [
    "Q3. generate함수의 인자로 사용한 temperature, top_k 값은 어떤 효과를 주는 옵션인가요?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa1c6aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "바람도 없는 공중에 수직의 파문을 내이며 고요히 떨어지는 오동잎은 누구의 발자취 입니까.\"\n",
      "\"그렇지. 그건 그렇고 말야.\"\n",
      "\"허허, 그게 무슨 소리요?\"\n",
      "그녀는 고개를 갸웃거렸다.\n",
      "\"무슨 소리야? 어디서 그런 소리가 들린단 말인가.\"\n",
      "그녀의 눈에서 눈물이 흘러내렸다.\n",
      "그녀가 고개를 끄덕였다.\n",
      "\"어디서 이런 소리를 들었단 말이냐!\"\n",
      "그러자 그녀의 눈이 번쩍 빛났다.\n",
      "\"아무리 생각해도 이상하지 않잖아. 내 생각에는, 너희들 중 누군가가 저런\n"
     ]
    }
   ],
   "source": [
    "# top_p sampling\n",
    "output_beam = model.generate(input_ids, max_length=max_length, num_beams=7, no_repeat_ngram_size=2,\n",
    "                             do_sample=True, top_p=0.90)\n",
    "print(tokenizer.decode(output_beam[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a53e39",
   "metadata": {},
   "source": [
    "Q4. top_p 인자의 기능은 무엇인가요?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f72d47",
   "metadata": {},
   "source": [
    "### kogpt-2에 RLHF 적용하기\n",
    "#### 데이터셋 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68673df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'prompt': '불고기용 고기 한우에요?',\n",
       "  'completion': \"'저는 인공지능 챗봇이며, 직접적으로 식품에 관한 정보를 가지고 있지 않습니다. 하지만 일반적으로 불고기용 고기는 한우, 쇠고기, 돼지고기 등 다양한 종류의 고기를 사용합니다. 하지만 한우는 대표적인 고급 육류로 알려져 있기 때문에, 한우를 사용하는 경우도 많습니다. 알러지나 개별 건강 상태에 따라 다를 수 있으니 충분한 정보 수집 후에 선택해 주시기 바랍니다.\",\n",
       "  'tokens': 193},\n",
       " {'prompt': '쓰던 앱이 유료로 전환됐어',\n",
       "  'completion': \"'어떤 앱인지 모르기 때문에 정확한 답변을 드리기 어렵습니다. 하지만, 일반적으로 유료 전환된 앱은 기존 무료 앱에서 추가적인 기능이나 서비스를 제공하는 경우가 많습니다. 이 경우, 유료 전환 이전 가격이 매우 저렴하거나 무료인 경우가 많으므로, 앱을 계속 사용하려면 추가적인 비용을 지불해야 할 수 있습니다. 하지만, 유료 전환 이후에는 기존 사용자 또는 새로운 사용자 모두가 추가 기능과 높은 품질의 서비스를 이용할 수 있게 됩니다. 따라서, 앱 개발자는 유료 모델로 전환함으로써 앱의 수익을 증가시키고 더 나은 서비스를 제공할 수 있습니다.\",\n",
       "  'tokens': 288},\n",
       " {'prompt': '여친이랑 다툼',\n",
       "  'completion': \"'저는 인공지능이기 때문에 여친과의 다툼에 대해서는 조언을 드릴 수는 없습니다. 하지만 다투게 되면 상대방의 말을 경청하는 태도와 서로의 감정을 존중하는 것이 중요합니다. 논리적이고 차분한 대화를 통해 문제를 해결해보세요. 그리고 서로를 이해하고 서로의 의견을 수용하는 것이 중요합니다.\",\n",
       "  'tokens': 153}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json \n",
    "import os \n",
    "\n",
    "data_path_1_SFT = os.getenv(\"HOME\")+'/aiffel/KoChatGPT/data_kochatgpt/kochatgpt_1_SFT.jsonl' \n",
    "with open(data_path_1_SFT, \"r\", encoding='utf-8-sig') as json_file:\n",
    "    list_data_dict = json.load(json_file)\n",
    "\n",
    "print(len(list_data_dict))\n",
    "list_data_dict[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf49741c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'prompt': '번디는 자신이 탐정잡지, 범죄소설 그리고 성범죄 관련 실제 범죄 다큐멘터리들을 탐독했다고 누구에게 말했나?'},\n",
       " {'prompt': '개포주공아파트는 몇 단지로 이루어져 있나?'},\n",
       " {'prompt': '김영삼의 후보 시절 지역표심을 겨냥한 발언을 문제삼은 후보는?'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PPO\n",
    "data_path_3_PPO = os.getenv(\"HOME\")+ '/aiffel/KoChatGPT/data_kochatgpt/kochatgpt_3_PPO.jsonl'\n",
    "with open(data_path_3_PPO, \"r\", encoding='utf-8-sig') as json_file:\n",
    "    list_data_dict = json.load(json_file)\n",
    "\n",
    "print(len(list_data_dict))\n",
    "list_data_dict[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44f74f1",
   "metadata": {},
   "source": [
    "## 2. Supervised Fine-Tuning\n",
    "### SFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0315741e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.optim import Adam\n",
    "from datasets import load_dataset\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from copy import deepcopy\n",
    "import copy\n",
    "import logging\n",
    "import json\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10e0dcee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2TokenizerFast(name_or_path='skt/kogpt2-base-v2', vocab_size=51200, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '</s>', 'eos_token': '</s>', 'unk_token': '</s>', 'pad_token': '</s>'}, clean_up_tokenization_spaces=True)\n"
     ]
    }
   ],
   "source": [
    "# model, tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained('skt/kogpt2-base-v2')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    'skt/kogpt2-base-v2', bos_token='</s>', eos_token='</s>', unk_token='</s>', pad_token='</s>',\n",
    "    padding_side=\"right\",\n",
    "    model_max_length=512,\n",
    ")\n",
    "\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8716e8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 인퍼런스 단계에서 사용할 prompt 딕셔너리 템플릿과 SFT 데이터셋 클래스를 정의\n",
    "from typing import Optional, Dict, Sequence\n",
    "\n",
    "class SFT_dataset(Dataset):\n",
    "\n",
    "    def __init__(self, data_path_1_SFT: str, tokenizer: transformers.PreTrainedTokenizer, verbose=False):\n",
    "        super(SFT_dataset, self).__init__()\n",
    "        logging.warning(\"Loading data...\")\n",
    "\n",
    "        pattern_instruction = 'prompt'  # instruction\n",
    "        pattern_output = 'completion'  # response\n",
    "\n",
    "        with open(data_path_1_SFT, \"r\", encoding='utf-8-sig') as json_file:\n",
    "            list_data_dict = json.load(json_file)\n",
    "\n",
    "        PROMPT_DICT = {\n",
    "            \"prompt_input\": (\n",
    "                \"### Instruction(명령어):\\n{prompt}\\n\\n### Response(응답):\"\n",
    "            )\n",
    "        }\n",
    "\n",
    "        prompt_input = PROMPT_DICT[\"prompt_input\"]\n",
    "\n",
    "        sources = []\n",
    "        for example in list_data_dict:\n",
    "            tmp = prompt_input.format_map(example)\n",
    "            sources.append(tmp)\n",
    "\n",
    "        targets = []\n",
    "        for example in list_data_dict:\n",
    "            targets.append(f\"{example[pattern_output]}{tokenizer.eos_token}\")\n",
    "        examples = [s + t for s, t in zip(sources, targets)]\n",
    "\n",
    "        sources_tokenized = self._tokenize_fn(sources, tokenizer)  # source\n",
    "        examples_tokenized = self._tokenize_fn(examples, tokenizer)  # source + target\n",
    "\n",
    "        input_ids = examples_tokenized[\"input_ids\"]\n",
    "        labels = copy.deepcopy(input_ids)\n",
    "        for label, source_len in zip(labels, sources_tokenized[\"input_ids_lens\"]):\n",
    "            label[:source_len] = -100\n",
    "\n",
    "        data_dict = dict(input_ids=input_ids, labels=labels)\n",
    "\n",
    "        self.input_ids = data_dict[\"input_ids\"]\n",
    "        self.labels = data_dict[\"labels\"]\n",
    "        logging.warning(\"Loading data done!!: %d\"%(len(self.labels)))\n",
    "\n",
    "\n",
    "    def _tokenize_fn(self, strings: Sequence[str], tokenizer: transformers.PreTrainedTokenizer) -> Dict:\n",
    "        tokenized_list = [\n",
    "            tokenizer(\n",
    "                text,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=\"longest\",\n",
    "                max_length=tokenizer.model_max_length,\n",
    "                truncation=True,\n",
    "            )\n",
    "            for text in strings\n",
    "        ]\n",
    "        input_ids = labels = [tokenized.input_ids[0] for tokenized in tokenized_list]\n",
    "        input_ids_lens = labels_lens = [\n",
    "            tokenized.input_ids.ne(tokenizer.pad_token_id).sum().item() for tokenized in tokenized_list\n",
    "        ]\n",
    "        return dict(\n",
    "            input_ids=input_ids,\n",
    "            labels=labels,\n",
    "            input_ids_lens=input_ids_lens,\n",
    "            labels_lens=labels_lens,\n",
    "        )\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "\n",
    "    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n",
    "        return dict(input_ids=self.input_ids[i], labels=self.labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "276cefc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorForSupervisedDataset(object): \n",
    "\n",
    "    tokenizer: transformers.PreTrainedTokenizer\n",
    "\n",
    "    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n",
    "        input_ids, labels = tuple([instance[key] for instance in instances] for key in (\"input_ids\", \"labels\"))\n",
    "        input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "            input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
    "        )\n",
    "        labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value= -100)\n",
    "        return dict(\n",
    "            input_ids=input_ids,\n",
    "            labels=labels,\n",
    "            attention_mask=input_ids.ne(self.tokenizer.pad_token_id),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "89deb23e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Loading data...\n",
      "WARNING:root:Loading data done!!: 12000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input : tensor([  739,   378,   378,   378, 14659, 13394, 37091, 10651,   383, 25841,\n",
      "         8006, 14914,   375,  7673, 20479,  8091, 22311,  9036, 30902, 13675,\n",
      "          375,   378,   378,   378, 41951,   454,  9549, 20549,   383,  8142,\n",
      "         7192, 14914,   382, 37767, 13753,  8263,  7166,   739,  8352,  7659,\n",
      "         9594, 25585, 13600,  8022,  9378, 11532,  9887, 11218,  9111, 16691,\n",
      "        10351, 10561,  9128, 20479,  8091,  9065,  9446,  9036, 28420, 26521,\n",
      "        10163, 26367,  6958,  9030,  9882, 12317, 25882,  9209, 37194, 10351,\n",
      "         9036, 12168, 10529, 15989,  9719, 15434, 10552, 11188, 13362,  9036,\n",
      "        15805, 11300, 11846,  9146, 16691,  9181,  7397, 15806, 13480, 11342,\n",
      "        17596,  9161, 19996,  9025, 25006, 18595,  9966, 12592, 10751, 11814,\n",
      "         8711,  9046, 12450,  9117,  7377, 12521,     1])\n",
      "output: tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,   382, 37767, 13753,  8263,  7166,   739,  8352,  7659,\n",
      "         9594, 25585, 13600,  8022,  9378, 11532,  9887, 11218,  9111, 16691,\n",
      "        10351, 10561,  9128, 20479,  8091,  9065,  9446,  9036, 28420, 26521,\n",
      "        10163, 26367,  6958,  9030,  9882, 12317, 25882,  9209, 37194, 10351,\n",
      "         9036, 12168, 10529, 15989,  9719, 15434, 10552, 11188, 13362,  9036,\n",
      "        15805, 11300, 11846,  9146, 16691,  9181,  7397, 15806, 13480, 11342,\n",
      "        17596,  9161, 19996,  9025, 25006, 18595,  9966, 12592, 10751, 11814,\n",
      "         8711,  9046, 12450,  9117,  7377, 12521,     1])\n"
     ]
    }
   ],
   "source": [
    "# data collator\n",
    "train_dataset = SFT_dataset(data_path_1_SFT=os.getenv(\"HOME\")+'/aiffel/KoChatGPT/data_kochatgpt/kochatgpt_1_SFT.jsonl', tokenizer=tokenizer)\n",
    "data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)\n",
    "\n",
    "print('input : %s'%train_dataset.input_ids[0])\n",
    "print('output: %s'%train_dataset.labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b0a921e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction(명령어):\n",
      "불고기용 고기 한우에요?\n",
      "\n",
      "### Response(응답):'저는 인공지능 챗봇이며, 직접적으로 식품에 관한 정보를 가지고 있지 않습니다. 하지만 일반적으로 불고기용 고기는 한우, 쇠고기, 돼지고기 등 다양한 종류의 고기를 사용합니다. 하지만 한우는 대표적인 고급 육류로 알려져 있기 때문에, 한우를 사용하는 경우도 많습니다. 알러지나 개별 건강 상태에 따라 다를 수 있으니 충분한 정보 수집 후에 선택해 주시기 바랍니다.</s><unk><unk> <unk>marketing.official.com/products/in\n"
     ]
    }
   ],
   "source": [
    "# 문제해결 - IndexError: too many indices for tensor of dimension 1\n",
    "\n",
    "# train_dataset.input_ids[0]를 리스트로 변환하여 2차원으로 만들기\n",
    "original_tensor = train_dataset.input_ids[0]\n",
    "\n",
    "# 원하는 shape으로 reshape\n",
    "reshaped_tensor = original_tensor.view(1, -1)\n",
    "\n",
    "output_beam_Q11 = model.generate(reshaped_tensor, max_length=max_length, num_beams=7, no_repeat_ngram_size=2,\n",
    "                             do_sample=True, top_p=0.90)\n",
    "print(tokenizer.decode(output_beam_Q11[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fd22fb",
   "metadata": {},
   "source": [
    "- 훈련을 위한 마지막 단계로 Training arguments를 사용해 trainer 클래스를 정의하겠습니다.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "31ea3805",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/aiffel/KoChatGPT/test\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=5,\n",
    "    prediction_loss_only=True,\n",
    "    fp16 = True\n",
    "    )\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9cc85826",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1500/1500 05:32, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.984100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.776800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.687200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()\n",
    "model.save_pretrained('/aiffel/KoChatGPT/output_1_SFT')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c0fe11",
   "metadata": {},
   "source": [
    "- 이제 문장 생성 능력을 확인하기 위해 허깅페이스의 pipleline 클래스를 사용하여 generator를 만들어보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "67edb88f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1219: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Instruction(명령어):\n",
      "불고기용 고기 한우에요?\n",
      "\n",
      "### Response(응답):'저는 인공지능 어시스턴트이기 때문에 불고기용 고기의 종류와 양에 대한 정보를 가지고 있지 않습니다. 하지만 일반적으로 불고기는 쇠고기와 함께 먹는 음식 중 하나입니다. 따라서 불고기를 먹을 수 있는 종류는 다양합니다. 예를 들어, 닭가슴살 스테이크, 오므라이스 샐러드 등이 있습니다.\n",
      "\n",
      "### Instruction(명령어):\n",
      "리처드 닉슨이 43대 부통령직을 수행한 년도는?\n",
      "\n",
      "### Response(응답):'리처드 닉슨은 42대 부통령직을 수행했습니다.作)作)은 \"리처드 닉슨\"이 41대 부통령을 수행한 년도를 가리키는 말입니다.作)는 \"리처드 닉슨\"이 40대 부통령을 맡았던 년도를 의미합니다.作은 \"리처드슨\"이 50대 부통령\n",
      "\n",
      "### Instruction(명령어):\n",
      "시카고 오헤어 국제공항은 어디에 있어?\n",
      "\n",
      "### Response(응답):'시카고 오 헤어 국제공항은 미국 캘리포니아주 샌프란시스코에 위치해 있습니다.子供共和國際空港)이라고 불립니다.子供公和国際空港이라는 뜻입니다.子供空和國際公港이라는 이름을 가진 항공사는 다음과 같습니다.\\n\\n1. 대한항공\n",
      "\n",
      "### Instruction(명령어):\n",
      "오늘 미세먼지 어때?\n",
      "\n",
      "### Response(응답):'저는 인공지능 챗봇으로써 미세먼지 정보를 알 수 없습니다. 미세먼지 예보를 확인해 보시는 것이 좋겠습니다.\\n\\n미세먼지 예보: 일반적으로 미세먼지는 주로 중국에서 발원하여 중국 전역으로 퍼져나가기 때문에 중국발 미세먼지가 유입될\n",
      "\n",
      "### Instruction(명령어):\n",
      "쓰던 앱이 유료로 전환됐어\n",
      "\n",
      "### Response(응답):'죄송합니다, 저는 인공지능 어시스턴트이기 때문에 유료로 전환된 앱에 대한 정보를 알 수 없습니다. 해당 앱의 공식 홈페이지나 앱을 확인해보시는 것을 추천드립니다. Young, Please, However, I do not\n",
      "\n",
      "### Instruction(명령어):\n",
      "여친이랑 다툼\n",
      "\n",
      "### Response(응답):'저는 AI 어시스턴트이기 때문에 감정을 느끼지는 않습니다. 하지만 일반적으로 여친과 갈등하는 것은 매우 자연스러운 일입니다. 그 이유는 여러 가지가 있을 수 있습니다.\\n\\n먼저, 상대방과의 대화를 통해 서로의 생각을 공유하고 소통하는 것이 중요합니다. 상대방과 대화를 통해 서로의 생각과 감정을 이해하는 것이 중요\n"
     ]
    }
   ],
   "source": [
    "generator = pipeline('text-generation', model='/aiffel/KoChatGPT/output_1_SFT', tokenizer=tokenizer)\n",
    "\n",
    "generation_args = dict(   \n",
    "    num_beams=4,\n",
    "    repetition_penalty=2.0,\n",
    "    no_repeat_ngram_size=4,\n",
    "    eos_token_id=375, # \\n   \n",
    "    max_new_tokens=64,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "PROMPT_DICT = {\n",
    "    \"prompt_input\": (\n",
    "        \"### Instruction(명령어):\\n{prompt}\\n\\n### Response(응답):\"\n",
    "    )\n",
    "}\n",
    "\n",
    "list_prompt = ['불고기용 고기 한우에요?',\n",
    "               '리처드 닉슨이 43대 부통령직을 수행한 년도는?',\n",
    "               '시카고 오헤어 국제공항은 어디에 있어?',\n",
    "               '오늘 미세먼지 어때?',\n",
    "              '쓰던 앱이 유료로 전환됐어',\n",
    "              '여친이랑 다툼']\n",
    "\n",
    "list_prompt = [PROMPT_DICT['prompt_input'].format_map({'prompt' : tmp}) for tmp in list_prompt]\n",
    "\n",
    "list_result = generator(list_prompt, **generation_args)   \n",
    "for prompt, result in zip(list_prompt, list_result):\n",
    "    print()\n",
    "    print((result[0]['generated_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "99cc0aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 캐시비우기\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b660cd",
   "metadata": {},
   "source": [
    "### 기존 KoGPT2와 SFT 적용 모델 결과 분석\n",
    "\n",
    "1. 응답의 길이 및 정보량\n",
    "- 기존 KoGPT2 모델은 대체로 짧은 응답을 생성하며, 질문에 대한 직접적인 답변을 제공하지 않는 경향이 있다. \n",
    "- 반면 SFT 적용 모델은 보다 긴 응답을 생성하며, 질문에 대해 보다 구체적이고 풍부한 정보를 제공하려 노력한다.\n",
    "\n",
    "2. 맥락 이해 및 일관성\n",
    "- 기존 KoGPT2 모델은 때로는 질문과 무관한 응답을 생성하거나, 문장 간 연결이 자연스럽지 않은 경우가 있다.\n",
    "- SFT 적용 모델은 질문의 맥락을 보다 잘 파악하고, 일관되고 자연스러운 응답을 생성하는 모습을 보인다. \n",
    "\n",
    "3. 윤리적/도덕적 판단\n",
    "- 기존 KoGPT2 모델은 때로는 부적절하거나 편향된 응답을 생성할 수 있다.\n",
    "- SFT 적용 모델은 자신이 AI 어시스턴트임을 밝히고, 개인적 견해 표명을 자제하는 등 보다 윤리적이고 중립적인 태도를 보인다.\n",
    "\n",
    "4. 전문 지식 및 사실 관계\n",
    "- 기존 KoGPT2 모델은 전문 분야나 사실 관계에 대해 부정확한 정보를 제공하는 경우가 있다. \n",
    "- SFT 적용 모델 역시 완벽하지는 않지만, 사실 관계에 대해 보다 정확한 정보를 제공하려 노력하는 모습을 보인다.\n",
    "\n",
    "**Overview**\n",
    "종합해보면, SFT 적용 모델이 기존 KoGPT2에 비해 보다 자연스럽고 정보량이 풍부한 응답을 생성하며, 질문의 맥락 파악과 일관성 면에서도 향상된 모습을 보입니다. 또한 응답의 윤리성과 사실 관계의 정확성 측면에서도 기존 모델 대비 발전이 있었다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5a3f74",
   "metadata": {},
   "source": [
    "## 3. Reward Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7449e921",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import Optional\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModel, AutoConfig\n",
    "from transformers.models.gpt2.configuration_gpt2 import GPT2Config\n",
    "from transformers.models.gpt2.modeling_gpt2 import GPT2Model\n",
    "import loralib as lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f55a7cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getenv(\"HOME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d4b5ef5c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.path.append(path)\n",
    "\n",
    "from chatgpt.dataset import RewardDataset\n",
    "from chatgpt.models.base import RewardModel\n",
    "from chatgpt.trainer import RewardModelTrainer\n",
    "from chatgpt.trainer.strategies import NaiveStrategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "603876b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/aiffel'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9b37aa53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/aiffel\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "40a97d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTRM_custom(RewardModel):\n",
    "\n",
    "    def __init__(self,\n",
    "                 pretrained: Optional[str] = None,\n",
    "                 config: Optional[GPT2Config] = None,\n",
    "                 checkpoint: bool = False,\n",
    "                 lora_rank: int = 0,\n",
    "                 lora_train_bias: str = 'none',\n",
    "                 tokenizer=None) -> None:\n",
    "        if pretrained is not None:\n",
    "            model = GPT2Model.from_pretrained(pretrained)\n",
    "            model.resize_token_embeddings(len(tokenizer))\n",
    "        elif config is not None:\n",
    "            model = GPT2Model(config)\n",
    "        else:\n",
    "            model = GPT2Model(GPT2Config())\n",
    "        if checkpoint:\n",
    "            model.gradient_checkpointing_enable()\n",
    "\n",
    "        value_head = nn.Linear(model.config.n_embd, 1)\n",
    "        super().__init__(model, value_head, lora_rank, lora_train_bias)\n",
    "\n",
    "        if pretrained is not None:\n",
    "            self.model = model\n",
    "            self.pretrained = pretrained\n",
    "\n",
    "\n",
    "    def save_pretrained(self, dir):\n",
    "        if self.pretrained is not None:\n",
    "            self.model.save_pretrained(dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a250e98c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at skt/kogpt2-base-v2 were not used when initializing GPT2Model: ['lm_head.weight']\n",
      "- This IS expected if you are initializing GPT2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPT2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained('skt/kogpt2-base-v2')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    'skt/kogpt2-base-v2', bos_token='</s>', eos_token='</s>', unk_token='</s>', pad_token='</s>',\n",
    "    padding_side=\"right\",\n",
    "    model_max_length=512,\n",
    ")\n",
    "\n",
    "with NaiveStrategy().model_init_context():\n",
    "        model = GPTRM_custom(pretrained='skt/kogpt2-base-v2', lora_rank=0, tokenizer=tokenizer).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f68e5859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before data num: 10220\n",
      "after  data num: 30660\n",
      "data example: \n",
      "{'prompt': '애플은 리사를 어떻게 처리했어', 'chosen': '애플이 누구인지 명확히 알 수 없어서, 리사가 누구인지와 어떤 상황에서 처리되었는지에 대한 추가적인 정보가 필요합니다. 따라서, 보다 정확한 답변을 제공할 수 없습니다.', 'rejected': '애플은 리사를 위해 고객 서비스 부서에서 고객 다양한 컴퓨터 관련 문제에 대해 응답하는 데 필요한 모든 지원을 제공했습니다. 사용자가 하드웨어 문제를 경험할 때, 전문가들은 필요한 수리(수리, 추가 부품 제공, 소프트웨어 업그레이드 등)을 제공해 드릴 수 있습니다. 또한, 사용자가 사용 방법 문제나 기타 문제를 경험할 때, 대화 상대로 사용자를 지원할 수 있는 전문 고객 서비스 직원들이 사용자에게 상담하고 도움을 주는 데 도움이 될 수 있는 정보를 제공합니다. 또한, 인터넷에서 제공되는 정보를 통해 문제를 해결하거나 고객 서비스 웹 사이트를 통해 자신의 문제를 진단할 수 있도록 하는 등 다양한 방법으로 리사를 처리해 왔습니다.'}\n"
     ]
    }
   ],
   "source": [
    "with open('/aiffel/KoChatGPT/data_kochatgpt/kochatgpt_2_RM.jsonl', \"r\", encoding='utf-8-sig') as json_file:\n",
    "    list_data_dict = json.load(json_file)\n",
    "\n",
    "total_data_ranking2chosen = []\n",
    "for tmp in list_data_dict:\n",
    "    one_data_ranking2chosen = []\n",
    "\n",
    "    data = {}\n",
    "    data['prompt'] = tmp['prompt']\n",
    "    if tmp['ranking'][0] < tmp['ranking'][1]:\n",
    "        data['chosen'] = tmp['completion_0']\n",
    "        data['rejected'] = tmp['completion_1']\n",
    "    else:\n",
    "        data['chosen'] = tmp['completion_1']\n",
    "        data['rejected'] = tmp['completion_0']\n",
    "    one_data_ranking2chosen.append(data)\n",
    "\n",
    "    data = {}\n",
    "    data['prompt'] = tmp['prompt']\n",
    "    if tmp['ranking'][0] < tmp['ranking'][2]:\n",
    "        data['chosen'] = tmp['completion_0']\n",
    "        data['rejected'] = tmp['completion_2']\n",
    "    else:\n",
    "        data['chosen'] = tmp['completion_2']\n",
    "        data['rejected'] = tmp['completion_0']\n",
    "    one_data_ranking2chosen.append(data)\n",
    "\n",
    "    data = {}\n",
    "    data['prompt'] = tmp['prompt']\n",
    "    if tmp['ranking'][1] < tmp['ranking'][2]:\n",
    "        data['chosen'] = tmp['completion_1']\n",
    "        data['rejected'] = tmp['completion_2']\n",
    "    else:\n",
    "        data['chosen'] = tmp['completion_2']\n",
    "        data['rejected'] = tmp['completion_1']\n",
    "    one_data_ranking2chosen.append(data)\n",
    "\n",
    "\n",
    "\n",
    "    total_data_ranking2chosen.extend(one_data_ranking2chosen)\n",
    "\n",
    "print('before data num: %d'%(len(list_data_dict)))\n",
    "print('after  data num: %d'%(len(total_data_ranking2chosen)))\n",
    "print('data example: \\n%s'%total_data_ranking2chosen[45])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104613dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9f4c107f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': '유아인이 류승완 감독을 만나 영화 베테랑의 시나리오를 받았던 곳은?', 'chosen': '유아인이 류승완 감독을 만나 영화 베테랑의 시나리오를 받았던 곳은 류승완의 사무실입니다.', 'rejected': '대구 영화사옥'}\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(230319)\n",
    "random.shuffle(total_data_ranking2chosen)\n",
    "print(total_data_ranking2chosen[45])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "48f43560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 1087.38it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 1077.81it/s]\n"
     ]
    }
   ],
   "source": [
    "train_data = total_data_ranking2chosen[:1000] \n",
    "eval_data = total_data_ranking2chosen[1000:1200]\n",
    "\n",
    "print(len(train_data))\n",
    "print(len(eval_data))\n",
    "\n",
    "train_dataset = RewardDataset(train_data, tokenizer, 512)\n",
    "eval_dataset = RewardDataset(eval_data, tokenizer, 512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c223e53",
   "metadata": {},
   "source": [
    "- 데이터셋이 잘 만들어졌는지 하나를 뽑아 확인해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8f659012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################################################################\n",
      "## prompt ##\n",
      "흑고래의 무게는 어느 정도야\n",
      "######################################################################\n",
      "## chosen ##\n",
      "흑고래의 평균 몸무게는 약 25~40톤 정도이지만, 최대 몸무게는 50톤 이상에 이를 수 있습니다.\n",
      "######################################################################\n",
      "## rejected ##\n",
      "흑고래의 무게는 매우 다양하게 달라집니다. 약 200kg에서 10톤까지 달라질 수 있습니다.\n"
     ]
    }
   ],
   "source": [
    "idx = 1\n",
    "print('#'*70)\n",
    "print('## prompt ##')\n",
    "print(train_data[idx]['prompt'])\n",
    "print('#'*70)\n",
    "print('## chosen ##')\n",
    "print(train_data[idx]['chosen'])\n",
    "print('#'*70)\n",
    "print('## rejected ##')\n",
    "print(train_data[idx]['rejected'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a34c9c",
   "metadata": {},
   "source": [
    "- (SFT 훈련때와 마찬가지로 RM 훈련시 많은 자원이 소모됩니다.\n",
    "모델 체크포인트를 활용할 수 있으니, 각각의 모델을 더 많은 데이터로 더 오래 훈련하고자 할 시,\n",
    "커널을 초기화 한 후 재학습을 해보세요.\n",
    "지금은 빠르게 학습해보기 위해 1epoch만 돌려보도록 하겠습니다)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "851fb369",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = RewardModelTrainer(model=model,\n",
    "                             strategy=NaiveStrategy(),\n",
    "                             optim=Adam(model.parameters(), lr=5e-5),\n",
    "                             train_dataset=train_dataset,\n",
    "                             eval_dataset=eval_dataset,\n",
    "                             batch_size=4,\n",
    "                             max_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0ba8c3ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Train step of epoch 0:   0%|          | 0/250 [00:00<?, ?it/s]\u001b[A\n",
      "Train step of epoch 0:   0%|          | 1/250 [00:00<03:43,  1.11it/s]\u001b[A\n",
      "Train step of epoch 0:   0%|          | 1/250 [00:00<03:43,  1.11it/s, loss=0.721]\u001b[A\n",
      "Train step of epoch 0:   1%|          | 2/250 [00:01<03:37,  1.14it/s, loss=0.721]\u001b[A\n",
      "Train step of epoch 0:   1%|          | 2/250 [00:01<03:37,  1.14it/s, loss=0.613]\u001b[A\n",
      "Train step of epoch 0:   1%|          | 3/250 [00:02<03:34,  1.15it/s, loss=0.613]\u001b[A\n",
      "Train step of epoch 0:   1%|          | 3/250 [00:02<03:34,  1.15it/s, loss=0.635]\u001b[A\n",
      "Train step of epoch 0:   2%|▏         | 4/250 [00:03<03:32,  1.16it/s, loss=0.635]\u001b[A\n",
      "Train step of epoch 0:   2%|▏         | 4/250 [00:03<03:32,  1.16it/s, loss=0.56] \u001b[A\n",
      "Train step of epoch 0:   2%|▏         | 5/250 [00:04<03:31,  1.16it/s, loss=0.56]\u001b[A\n",
      "Train step of epoch 0:   2%|▏         | 5/250 [00:04<03:31,  1.16it/s, loss=0.328]\u001b[A\n",
      "Train step of epoch 0:   2%|▏         | 6/250 [00:05<03:29,  1.16it/s, loss=0.328]\u001b[A\n",
      "Train step of epoch 0:   2%|▏         | 6/250 [00:05<03:29,  1.16it/s, loss=0.287]\u001b[A\n",
      "Train step of epoch 0:   3%|▎         | 7/250 [00:06<03:29,  1.16it/s, loss=0.287]\u001b[A\n",
      "Train step of epoch 0:   3%|▎         | 7/250 [00:06<03:29,  1.16it/s, loss=0.588]\u001b[A\n",
      "Train step of epoch 0:   3%|▎         | 8/250 [00:06<03:28,  1.16it/s, loss=0.588]\u001b[A\n",
      "Train step of epoch 0:   3%|▎         | 8/250 [00:06<03:28,  1.16it/s, loss=2.25] \u001b[A\n",
      "Train step of epoch 0:   4%|▎         | 9/250 [00:07<03:27,  1.16it/s, loss=2.25]\u001b[A\n",
      "Train step of epoch 0:   4%|▎         | 9/250 [00:07<03:27,  1.16it/s, loss=0.368]\u001b[A\n",
      "Train step of epoch 0:   4%|▍         | 10/250 [00:08<03:27,  1.16it/s, loss=0.368]\u001b[A\n",
      "Train step of epoch 0:   4%|▍         | 10/250 [00:08<03:27,  1.16it/s, loss=0.622]\u001b[A\n",
      "Train step of epoch 0:   4%|▍         | 11/250 [00:09<03:26,  1.16it/s, loss=0.622]\u001b[A\n",
      "Train step of epoch 0:   4%|▍         | 11/250 [00:09<03:26,  1.16it/s, loss=0.396]\u001b[A\n",
      "Train step of epoch 0:   5%|▍         | 12/250 [00:10<03:25,  1.16it/s, loss=0.396]\u001b[A\n",
      "Train step of epoch 0:   5%|▍         | 12/250 [00:10<03:25,  1.16it/s, loss=0.379]\u001b[A\n",
      "Train step of epoch 0:   5%|▌         | 13/250 [00:11<03:25,  1.15it/s, loss=0.379]\u001b[A\n",
      "Train step of epoch 0:   5%|▌         | 13/250 [00:11<03:25,  1.15it/s, loss=0.0365]\u001b[A\n",
      "Train step of epoch 0:   6%|▌         | 14/250 [00:12<03:24,  1.15it/s, loss=0.0365]\u001b[A\n",
      "Train step of epoch 0:   6%|▌         | 14/250 [00:12<03:24,  1.15it/s, loss=1.51]  \u001b[A\n",
      "Train step of epoch 0:   6%|▌         | 15/250 [00:12<03:23,  1.15it/s, loss=1.51]\u001b[A\n",
      "Train step of epoch 0:   6%|▌         | 15/250 [00:13<03:23,  1.15it/s, loss=0.543]\u001b[A\n",
      "Train step of epoch 0:   6%|▋         | 16/250 [00:13<03:23,  1.15it/s, loss=0.543]\u001b[A\n",
      "Train step of epoch 0:   6%|▋         | 16/250 [00:13<03:23,  1.15it/s, loss=1.63] \u001b[A\n",
      "Train step of epoch 0:   7%|▋         | 17/250 [00:14<03:22,  1.15it/s, loss=1.63]\u001b[A\n",
      "Train step of epoch 0:   7%|▋         | 17/250 [00:14<03:22,  1.15it/s, loss=0.608]\u001b[A\n",
      "Train step of epoch 0:   7%|▋         | 18/250 [00:15<03:22,  1.15it/s, loss=0.608]\u001b[A\n",
      "Train step of epoch 0:   7%|▋         | 18/250 [00:15<03:22,  1.15it/s, loss=0.455]\u001b[A\n",
      "Train step of epoch 0:   8%|▊         | 19/250 [00:16<03:21,  1.15it/s, loss=0.455]\u001b[A\n",
      "Train step of epoch 0:   8%|▊         | 19/250 [00:16<03:21,  1.15it/s, loss=1.01] \u001b[A\n",
      "Train step of epoch 0:   8%|▊         | 20/250 [00:17<03:20,  1.15it/s, loss=1.01]\u001b[A\n",
      "Train step of epoch 0:   8%|▊         | 20/250 [00:17<03:20,  1.15it/s, loss=0.624]\u001b[A\n",
      "Train step of epoch 0:   8%|▊         | 21/250 [00:18<03:20,  1.14it/s, loss=0.624]\u001b[A\n",
      "Train step of epoch 0:   8%|▊         | 21/250 [00:18<03:20,  1.14it/s, loss=0.775]\u001b[A\n",
      "Train step of epoch 0:   9%|▉         | 22/250 [00:19<03:19,  1.14it/s, loss=0.775]\u001b[A\n",
      "Train step of epoch 0:   9%|▉         | 22/250 [00:19<03:19,  1.14it/s, loss=0.632]\u001b[A\n",
      "Train step of epoch 0:   9%|▉         | 23/250 [00:19<03:18,  1.14it/s, loss=0.632]\u001b[A\n",
      "Train step of epoch 0:   9%|▉         | 23/250 [00:20<03:18,  1.14it/s, loss=0.871]\u001b[A\n",
      "Train step of epoch 0:  10%|▉         | 24/250 [00:20<03:18,  1.14it/s, loss=0.871]\u001b[A\n",
      "Train step of epoch 0:  10%|▉         | 24/250 [00:20<03:18,  1.14it/s, loss=0.608]\u001b[A\n",
      "Train step of epoch 0:  10%|█         | 25/250 [00:21<03:17,  1.14it/s, loss=0.608]\u001b[A\n",
      "Train step of epoch 0:  10%|█         | 25/250 [00:21<03:17,  1.14it/s, loss=0.536]\u001b[A\n",
      "Train step of epoch 0:  10%|█         | 26/250 [00:22<03:16,  1.14it/s, loss=0.536]\u001b[A\n",
      "Train step of epoch 0:  10%|█         | 26/250 [00:22<03:16,  1.14it/s, loss=0.677]\u001b[A\n",
      "Train step of epoch 0:  11%|█         | 27/250 [00:23<03:15,  1.14it/s, loss=0.677]\u001b[A\n",
      "Train step of epoch 0:  11%|█         | 27/250 [00:23<03:15,  1.14it/s, loss=0.635]\u001b[A\n",
      "Train step of epoch 0:  11%|█         | 28/250 [00:24<03:15,  1.14it/s, loss=0.635]\u001b[A\n",
      "Train step of epoch 0:  11%|█         | 28/250 [00:24<03:15,  1.14it/s, loss=0.566]\u001b[A\n",
      "Train step of epoch 0:  12%|█▏        | 29/250 [00:25<03:14,  1.14it/s, loss=0.566]\u001b[A\n",
      "Train step of epoch 0:  12%|█▏        | 29/250 [00:25<03:14,  1.14it/s, loss=0.51] \u001b[A\n",
      "Train step of epoch 0:  12%|█▏        | 30/250 [00:26<03:13,  1.13it/s, loss=0.51]\u001b[A\n",
      "Train step of epoch 0:  12%|█▏        | 30/250 [00:26<03:13,  1.13it/s, loss=0.438]\u001b[A\n",
      "Train step of epoch 0:  12%|█▏        | 31/250 [00:27<03:13,  1.13it/s, loss=0.438]\u001b[A\n",
      "Train step of epoch 0:  12%|█▏        | 31/250 [00:27<03:13,  1.13it/s, loss=0.353]\u001b[A\n",
      "Train step of epoch 0:  13%|█▎        | 32/250 [00:27<03:12,  1.13it/s, loss=0.353]\u001b[A\n",
      "Train step of epoch 0:  13%|█▎        | 32/250 [00:27<03:12,  1.13it/s, loss=0.69] \u001b[A\n",
      "Train step of epoch 0:  13%|█▎        | 33/250 [00:28<03:11,  1.13it/s, loss=0.69]\u001b[A\n",
      "Train step of epoch 0:  13%|█▎        | 33/250 [00:28<03:11,  1.13it/s, loss=0.402]\u001b[A\n",
      "Train step of epoch 0:  14%|█▎        | 34/250 [00:29<03:11,  1.13it/s, loss=0.402]\u001b[A\n",
      "Train step of epoch 0:  14%|█▎        | 34/250 [00:29<03:11,  1.13it/s, loss=0.429]\u001b[A\n",
      "Train step of epoch 0:  14%|█▍        | 35/250 [00:30<03:10,  1.13it/s, loss=0.429]\u001b[A\n",
      "Train step of epoch 0:  14%|█▍        | 35/250 [00:30<03:10,  1.13it/s, loss=0.727]\u001b[A\n",
      "Train step of epoch 0:  14%|█▍        | 36/250 [00:31<03:10,  1.13it/s, loss=0.727]\u001b[A\n",
      "Train step of epoch 0:  14%|█▍        | 36/250 [00:31<03:10,  1.13it/s, loss=1.67] \u001b[A\n",
      "Train step of epoch 0:  15%|█▍        | 37/250 [00:32<03:09,  1.12it/s, loss=1.67]\u001b[A\n",
      "Train step of epoch 0:  15%|█▍        | 37/250 [00:32<03:09,  1.12it/s, loss=0.946]\u001b[A\n",
      "Train step of epoch 0:  15%|█▌        | 38/250 [00:33<03:09,  1.12it/s, loss=0.946]\u001b[A\n",
      "Train step of epoch 0:  15%|█▌        | 38/250 [00:33<03:09,  1.12it/s, loss=0.671]\u001b[A\n",
      "Train step of epoch 0:  16%|█▌        | 39/250 [00:34<03:08,  1.12it/s, loss=0.671]\u001b[A\n",
      "Train step of epoch 0:  16%|█▌        | 39/250 [00:34<03:08,  1.12it/s, loss=0.918]\u001b[A\n",
      "Train step of epoch 0:  16%|█▌        | 40/250 [00:35<03:07,  1.12it/s, loss=0.918]\u001b[A\n",
      "Train step of epoch 0:  16%|█▌        | 40/250 [00:35<03:07,  1.12it/s, loss=0.444]\u001b[A\n",
      "Train step of epoch 0:  16%|█▋        | 41/250 [00:35<03:07,  1.12it/s, loss=0.444]\u001b[A\n",
      "Train step of epoch 0:  16%|█▋        | 41/250 [00:35<03:07,  1.12it/s, loss=0.385]\u001b[A\n",
      "Train step of epoch 0:  17%|█▋        | 42/250 [00:36<03:06,  1.12it/s, loss=0.385]\u001b[A\n",
      "Train step of epoch 0:  17%|█▋        | 42/250 [00:36<03:06,  1.12it/s, loss=0.582]\u001b[A\n",
      "Train step of epoch 0:  17%|█▋        | 43/250 [00:37<03:05,  1.12it/s, loss=0.582]\u001b[A\n",
      "Train step of epoch 0:  17%|█▋        | 43/250 [00:37<03:05,  1.12it/s, loss=0.806]\u001b[A\n",
      "Train step of epoch 0:  18%|█▊        | 44/250 [00:38<03:04,  1.12it/s, loss=0.806]\u001b[A\n",
      "Train step of epoch 0:  18%|█▊        | 44/250 [00:38<03:04,  1.12it/s, loss=0.977]\u001b[A\n",
      "Train step of epoch 0:  18%|█▊        | 45/250 [00:39<03:03,  1.11it/s, loss=0.977]\u001b[A\n",
      "Train step of epoch 0:  18%|█▊        | 45/250 [00:39<03:03,  1.11it/s, loss=0.608]\u001b[A\n",
      "Train step of epoch 0:  18%|█▊        | 46/250 [00:40<03:03,  1.11it/s, loss=0.608]\u001b[A\n",
      "Train step of epoch 0:  18%|█▊        | 46/250 [00:40<03:03,  1.11it/s, loss=0.567]\u001b[A\n",
      "Train step of epoch 0:  19%|█▉        | 47/250 [00:41<03:03,  1.11it/s, loss=0.567]\u001b[A\n",
      "Train step of epoch 0:  19%|█▉        | 47/250 [00:41<03:03,  1.11it/s, loss=0.673]\u001b[A\n",
      "Train step of epoch 0:  19%|█▉        | 48/250 [00:42<03:02,  1.11it/s, loss=0.673]\u001b[A\n",
      "Train step of epoch 0:  19%|█▉        | 48/250 [00:42<03:02,  1.11it/s, loss=0.764]\u001b[A\n",
      "Train step of epoch 0:  20%|█▉        | 49/250 [00:43<03:01,  1.10it/s, loss=0.764]\u001b[A\n",
      "Train step of epoch 0:  20%|█▉        | 49/250 [00:43<03:01,  1.10it/s, loss=0.636]\u001b[A\n",
      "Train step of epoch 0:  20%|██        | 50/250 [00:44<03:01,  1.10it/s, loss=0.636]\u001b[A\n",
      "Train step of epoch 0:  20%|██        | 50/250 [00:44<03:01,  1.10it/s, loss=0.845]\u001b[A\n",
      "Train step of epoch 0:  20%|██        | 51/250 [00:45<03:00,  1.10it/s, loss=0.845]\u001b[A\n",
      "Train step of epoch 0:  20%|██        | 51/250 [00:45<03:00,  1.10it/s, loss=0.501]\u001b[A\n",
      "Train step of epoch 0:  21%|██        | 52/250 [00:45<02:59,  1.10it/s, loss=0.501]\u001b[A\n",
      "Train step of epoch 0:  21%|██        | 52/250 [00:45<02:59,  1.10it/s, loss=0.646]\u001b[A\n",
      "Train step of epoch 0:  21%|██        | 53/250 [00:46<02:59,  1.10it/s, loss=0.646]\u001b[A\n",
      "Train step of epoch 0:  21%|██        | 53/250 [00:46<02:59,  1.10it/s, loss=0.475]\u001b[A\n",
      "Train step of epoch 0:  22%|██▏       | 54/250 [00:47<02:58,  1.10it/s, loss=0.475]\u001b[A\n",
      "Train step of epoch 0:  22%|██▏       | 54/250 [00:47<02:58,  1.10it/s, loss=0.437]\u001b[A\n",
      "Train step of epoch 0:  22%|██▏       | 55/250 [00:48<02:57,  1.10it/s, loss=0.437]\u001b[A\n",
      "Train step of epoch 0:  22%|██▏       | 55/250 [00:48<02:57,  1.10it/s, loss=0.599]\u001b[A\n",
      "Train step of epoch 0:  22%|██▏       | 56/250 [00:49<02:57,  1.09it/s, loss=0.599]\u001b[A\n",
      "Train step of epoch 0:  22%|██▏       | 56/250 [00:49<02:57,  1.09it/s, loss=0.601]\u001b[A\n",
      "Train step of epoch 0:  23%|██▎       | 57/250 [00:50<02:56,  1.09it/s, loss=0.601]\u001b[A\n",
      "Train step of epoch 0:  23%|██▎       | 57/250 [00:50<02:56,  1.09it/s, loss=0.602]\u001b[A\n",
      "Train step of epoch 0:  23%|██▎       | 58/250 [00:51<02:56,  1.09it/s, loss=0.602]\u001b[A\n",
      "Train step of epoch 0:  23%|██▎       | 58/250 [00:51<02:56,  1.09it/s, loss=0.555]\u001b[A\n",
      "Train step of epoch 0:  24%|██▎       | 59/250 [00:52<02:55,  1.09it/s, loss=0.555]\u001b[A\n",
      "Train step of epoch 0:  24%|██▎       | 59/250 [00:52<02:55,  1.09it/s, loss=0.56] \u001b[A\n",
      "Train step of epoch 0:  24%|██▍       | 60/250 [00:53<02:54,  1.09it/s, loss=0.56]\u001b[A\n",
      "Train step of epoch 0:  24%|██▍       | 60/250 [00:53<02:54,  1.09it/s, loss=0.378]\u001b[A\n",
      "Train step of epoch 0:  24%|██▍       | 61/250 [00:54<02:53,  1.09it/s, loss=0.378]\u001b[A\n",
      "Train step of epoch 0:  24%|██▍       | 61/250 [00:54<02:53,  1.09it/s, loss=0.928]\u001b[A\n",
      "Train step of epoch 0:  25%|██▍       | 62/250 [00:55<02:52,  1.09it/s, loss=0.928]\u001b[A\n",
      "Train step of epoch 0:  25%|██▍       | 62/250 [00:55<02:52,  1.09it/s, loss=0.228]\u001b[A\n",
      "Train step of epoch 0:  25%|██▌       | 63/250 [00:56<02:52,  1.09it/s, loss=0.228]\u001b[A\n",
      "Train step of epoch 0:  25%|██▌       | 63/250 [00:56<02:52,  1.09it/s, loss=0.924]\u001b[A\n",
      "Train step of epoch 0:  26%|██▌       | 64/250 [00:56<02:51,  1.09it/s, loss=0.924]\u001b[A\n",
      "Train step of epoch 0:  26%|██▌       | 64/250 [00:56<02:51,  1.09it/s, loss=0.324]\u001b[A\n",
      "Train step of epoch 0:  26%|██▌       | 65/250 [00:57<02:50,  1.09it/s, loss=0.324]\u001b[A\n",
      "Train step of epoch 0:  26%|██▌       | 65/250 [00:57<02:50,  1.09it/s, loss=0.741]\u001b[A\n",
      "Train step of epoch 0:  26%|██▋       | 66/250 [00:58<02:49,  1.09it/s, loss=0.741]\u001b[A\n",
      "Train step of epoch 0:  26%|██▋       | 66/250 [00:58<02:49,  1.09it/s, loss=0.16] \u001b[A\n",
      "Train step of epoch 0:  27%|██▋       | 67/250 [00:59<02:48,  1.09it/s, loss=0.16]\u001b[A\n",
      "Train step of epoch 0:  27%|██▋       | 67/250 [00:59<02:48,  1.09it/s, loss=0.901]\u001b[A\n",
      "Train step of epoch 0:  27%|██▋       | 68/250 [01:00<02:47,  1.09it/s, loss=0.901]\u001b[A\n",
      "Train step of epoch 0:  27%|██▋       | 68/250 [01:00<02:47,  1.09it/s, loss=0.332]\u001b[A\n",
      "Train step of epoch 0:  28%|██▊       | 69/250 [01:01<02:46,  1.09it/s, loss=0.332]\u001b[A\n",
      "Train step of epoch 0:  28%|██▊       | 69/250 [01:01<02:46,  1.09it/s, loss=0.814]\u001b[A\n",
      "Train step of epoch 0:  28%|██▊       | 70/250 [01:02<02:44,  1.09it/s, loss=0.814]\u001b[A\n",
      "Train step of epoch 0:  28%|██▊       | 70/250 [01:02<02:44,  1.09it/s, loss=0.816]\u001b[A\n",
      "Train step of epoch 0:  28%|██▊       | 71/250 [01:03<02:43,  1.09it/s, loss=0.816]\u001b[A\n",
      "Train step of epoch 0:  28%|██▊       | 71/250 [01:03<02:43,  1.09it/s, loss=0.495]\u001b[A\n",
      "Train step of epoch 0:  29%|██▉       | 72/250 [01:04<02:42,  1.10it/s, loss=0.495]\u001b[A\n",
      "Train step of epoch 0:  29%|██▉       | 72/250 [01:04<02:42,  1.10it/s, loss=0.801]\u001b[A\n",
      "Train step of epoch 0:  29%|██▉       | 73/250 [01:05<02:41,  1.10it/s, loss=0.801]\u001b[A\n",
      "Train step of epoch 0:  29%|██▉       | 73/250 [01:05<02:41,  1.10it/s, loss=0.796]\u001b[A\n",
      "Train step of epoch 0:  30%|██▉       | 74/250 [01:06<02:40,  1.10it/s, loss=0.796]\u001b[A\n",
      "Train step of epoch 0:  30%|██▉       | 74/250 [01:06<02:40,  1.10it/s, loss=0.538]\u001b[A\n",
      "Train step of epoch 0:  30%|███       | 75/250 [01:06<02:38,  1.10it/s, loss=0.538]\u001b[A\n",
      "Train step of epoch 0:  30%|███       | 75/250 [01:06<02:38,  1.10it/s, loss=0.593]\u001b[A\n",
      "Train step of epoch 0:  30%|███       | 76/250 [01:07<02:37,  1.10it/s, loss=0.593]\u001b[A\n",
      "Train step of epoch 0:  30%|███       | 76/250 [01:07<02:37,  1.10it/s, loss=0.737]\u001b[A\n",
      "Train step of epoch 0:  31%|███       | 77/250 [01:08<02:36,  1.10it/s, loss=0.737]\u001b[A\n",
      "Train step of epoch 0:  31%|███       | 77/250 [01:08<02:36,  1.10it/s, loss=0.439]\u001b[A\n",
      "Train step of epoch 0:  31%|███       | 78/250 [01:09<02:35,  1.10it/s, loss=0.439]\u001b[A\n",
      "Train step of epoch 0:  31%|███       | 78/250 [01:09<02:35,  1.10it/s, loss=0.645]\u001b[A\n",
      "Train step of epoch 0:  32%|███▏      | 79/250 [01:10<02:34,  1.11it/s, loss=0.645]\u001b[A\n",
      "Train step of epoch 0:  32%|███▏      | 79/250 [01:10<02:34,  1.11it/s, loss=0.545]\u001b[A\n",
      "Train step of epoch 0:  32%|███▏      | 80/250 [01:11<02:33,  1.11it/s, loss=0.545]\u001b[A\n",
      "Train step of epoch 0:  32%|███▏      | 80/250 [01:11<02:33,  1.11it/s, loss=0.694]\u001b[A\n",
      "Train step of epoch 0:  32%|███▏      | 81/250 [01:12<02:31,  1.11it/s, loss=0.694]\u001b[A\n",
      "Train step of epoch 0:  32%|███▏      | 81/250 [01:12<02:31,  1.11it/s, loss=0.377]\u001b[A\n",
      "Train step of epoch 0:  33%|███▎      | 82/250 [01:13<02:30,  1.11it/s, loss=0.377]\u001b[A\n",
      "Train step of epoch 0:  33%|███▎      | 82/250 [01:13<02:30,  1.11it/s, loss=0.823]\u001b[A\n",
      "Train step of epoch 0:  33%|███▎      | 83/250 [01:14<02:29,  1.11it/s, loss=0.823]\u001b[A\n",
      "Train step of epoch 0:  33%|███▎      | 83/250 [01:14<02:29,  1.11it/s, loss=0.588]\u001b[A\n",
      "Train step of epoch 0:  34%|███▎      | 84/250 [01:15<02:28,  1.11it/s, loss=0.588]\u001b[A\n",
      "Train step of epoch 0:  34%|███▎      | 84/250 [01:15<02:28,  1.11it/s, loss=0.607]\u001b[A\n",
      "Train step of epoch 0:  34%|███▍      | 85/250 [01:15<02:27,  1.12it/s, loss=0.607]\u001b[A\n",
      "Train step of epoch 0:  34%|███▍      | 85/250 [01:15<02:27,  1.12it/s, loss=0.678]\u001b[A\n",
      "Train step of epoch 0:  34%|███▍      | 86/250 [01:16<02:26,  1.12it/s, loss=0.678]\u001b[A\n",
      "Train step of epoch 0:  34%|███▍      | 86/250 [01:16<02:26,  1.12it/s, loss=0.558]\u001b[A\n",
      "Train step of epoch 0:  35%|███▍      | 87/250 [01:17<02:25,  1.12it/s, loss=0.558]\u001b[A\n",
      "Train step of epoch 0:  35%|███▍      | 87/250 [01:17<02:25,  1.12it/s, loss=0.488]\u001b[A\n",
      "Train step of epoch 0:  35%|███▌      | 88/250 [01:18<02:24,  1.12it/s, loss=0.488]\u001b[A\n",
      "Train step of epoch 0:  35%|███▌      | 88/250 [01:18<02:24,  1.12it/s, loss=0.903]\u001b[A\n",
      "Train step of epoch 0:  36%|███▌      | 89/250 [01:19<02:23,  1.12it/s, loss=0.903]\u001b[A\n",
      "Train step of epoch 0:  36%|███▌      | 89/250 [01:19<02:23,  1.12it/s, loss=0.824]\u001b[A\n",
      "Train step of epoch 0:  36%|███▌      | 90/250 [01:20<02:22,  1.12it/s, loss=0.824]\u001b[A\n",
      "Train step of epoch 0:  36%|███▌      | 90/250 [01:20<02:22,  1.12it/s, loss=0.701]\u001b[A\n",
      "Train step of epoch 0:  36%|███▋      | 91/250 [01:21<02:21,  1.12it/s, loss=0.701]\u001b[A\n",
      "Train step of epoch 0:  36%|███▋      | 91/250 [01:21<02:21,  1.12it/s, loss=0.625]\u001b[A\n",
      "Train step of epoch 0:  37%|███▋      | 92/250 [01:22<02:20,  1.12it/s, loss=0.625]\u001b[A\n",
      "Train step of epoch 0:  37%|███▋      | 92/250 [01:22<02:20,  1.12it/s, loss=0.6]  \u001b[A\n",
      "Train step of epoch 0:  37%|███▋      | 93/250 [01:23<02:19,  1.12it/s, loss=0.6]\u001b[A\n",
      "Train step of epoch 0:  37%|███▋      | 93/250 [01:23<02:19,  1.12it/s, loss=0.852]\u001b[A\n",
      "Train step of epoch 0:  38%|███▊      | 94/250 [01:23<02:18,  1.12it/s, loss=0.852]\u001b[A\n",
      "Train step of epoch 0:  38%|███▊      | 94/250 [01:23<02:18,  1.12it/s, loss=0.729]\u001b[A\n",
      "Train step of epoch 0:  38%|███▊      | 95/250 [01:24<02:17,  1.12it/s, loss=0.729]\u001b[A\n",
      "Train step of epoch 0:  38%|███▊      | 95/250 [01:24<02:17,  1.12it/s, loss=0.495]\u001b[A\n",
      "Train step of epoch 0:  38%|███▊      | 96/250 [01:25<02:16,  1.13it/s, loss=0.495]\u001b[A\n",
      "Train step of epoch 0:  38%|███▊      | 96/250 [01:25<02:16,  1.13it/s, loss=0.687]\u001b[A\n",
      "Train step of epoch 0:  39%|███▉      | 97/250 [01:26<02:15,  1.13it/s, loss=0.687]\u001b[A\n",
      "Train step of epoch 0:  39%|███▉      | 97/250 [01:26<02:15,  1.13it/s, loss=0.787]\u001b[A\n",
      "Train step of epoch 0:  39%|███▉      | 98/250 [01:27<02:14,  1.13it/s, loss=0.787]\u001b[A\n",
      "Train step of epoch 0:  39%|███▉      | 98/250 [01:27<02:14,  1.13it/s, loss=0.811]\u001b[A\n",
      "Train step of epoch 0:  40%|███▉      | 99/250 [01:28<02:14,  1.13it/s, loss=0.811]\u001b[A\n",
      "Train step of epoch 0:  40%|███▉      | 99/250 [01:28<02:14,  1.13it/s, loss=0.597]\u001b[A\n",
      "Train step of epoch 0:  40%|████      | 100/250 [01:29<02:13,  1.13it/s, loss=0.597]\u001b[A\n",
      "Train step of epoch 0:  40%|████      | 100/250 [01:29<02:13,  1.13it/s, loss=0.613]\u001b[A\n",
      "Train step of epoch 0:  40%|████      | 101/250 [01:30<02:12,  1.13it/s, loss=0.613]\u001b[A\n",
      "Train step of epoch 0:  40%|████      | 101/250 [01:30<02:12,  1.13it/s, loss=0.678]\u001b[A\n",
      "Train step of epoch 0:  41%|████      | 102/250 [01:31<02:10,  1.13it/s, loss=0.678]\u001b[A\n",
      "Train step of epoch 0:  41%|████      | 102/250 [01:31<02:10,  1.13it/s, loss=0.655]\u001b[A\n",
      "Train step of epoch 0:  41%|████      | 103/250 [01:31<02:10,  1.13it/s, loss=0.655]\u001b[A\n",
      "Train step of epoch 0:  41%|████      | 103/250 [01:31<02:10,  1.13it/s, loss=0.732]\u001b[A\n",
      "Train step of epoch 0:  42%|████▏     | 104/250 [01:32<02:09,  1.13it/s, loss=0.732]\u001b[A\n",
      "Train step of epoch 0:  42%|████▏     | 104/250 [01:32<02:09,  1.13it/s, loss=0.765]\u001b[A\n",
      "Train step of epoch 0:  42%|████▏     | 105/250 [01:33<02:08,  1.13it/s, loss=0.765]\u001b[A\n",
      "Train step of epoch 0:  42%|████▏     | 105/250 [01:33<02:08,  1.13it/s, loss=0.657]\u001b[A\n",
      "Train step of epoch 0:  42%|████▏     | 106/250 [01:34<02:07,  1.13it/s, loss=0.657]\u001b[A\n",
      "Train step of epoch 0:  42%|████▏     | 106/250 [01:34<02:07,  1.13it/s, loss=0.732]\u001b[A\n",
      "Train step of epoch 0:  43%|████▎     | 107/250 [01:35<02:06,  1.13it/s, loss=0.732]\u001b[A\n",
      "Train step of epoch 0:  43%|████▎     | 107/250 [01:35<02:06,  1.13it/s, loss=0.725]\u001b[A\n",
      "Train step of epoch 0:  43%|████▎     | 108/250 [01:36<02:05,  1.13it/s, loss=0.725]\u001b[A\n",
      "Train step of epoch 0:  43%|████▎     | 108/250 [01:36<02:05,  1.13it/s, loss=0.603]\u001b[A\n",
      "Train step of epoch 0:  44%|████▎     | 109/250 [01:37<02:04,  1.13it/s, loss=0.603]\u001b[A\n",
      "Train step of epoch 0:  44%|████▎     | 109/250 [01:37<02:04,  1.13it/s, loss=0.688]\u001b[A\n",
      "Train step of epoch 0:  44%|████▍     | 110/250 [01:38<02:03,  1.13it/s, loss=0.688]\u001b[A\n",
      "Train step of epoch 0:  44%|████▍     | 110/250 [01:38<02:03,  1.13it/s, loss=0.705]\u001b[A\n",
      "Train step of epoch 0:  44%|████▍     | 111/250 [01:39<02:02,  1.13it/s, loss=0.705]\u001b[A\n",
      "Train step of epoch 0:  44%|████▍     | 111/250 [01:39<02:02,  1.13it/s, loss=0.598]\u001b[A\n",
      "Train step of epoch 0:  45%|████▍     | 112/250 [01:39<02:02,  1.13it/s, loss=0.598]\u001b[A\n",
      "Train step of epoch 0:  45%|████▍     | 112/250 [01:39<02:02,  1.13it/s, loss=0.645]\u001b[A\n",
      "Train step of epoch 0:  45%|████▌     | 113/250 [01:40<02:01,  1.13it/s, loss=0.645]\u001b[A\n",
      "Train step of epoch 0:  45%|████▌     | 113/250 [01:40<02:01,  1.13it/s, loss=0.68] \u001b[A\n",
      "Train step of epoch 0:  46%|████▌     | 114/250 [01:41<02:00,  1.13it/s, loss=0.68]\u001b[A\n",
      "Train step of epoch 0:  46%|████▌     | 114/250 [01:41<02:00,  1.13it/s, loss=0.834]\u001b[A\n",
      "Train step of epoch 0:  46%|████▌     | 115/250 [01:42<01:59,  1.13it/s, loss=0.834]\u001b[A\n",
      "Train step of epoch 0:  46%|████▌     | 115/250 [01:42<01:59,  1.13it/s, loss=0.677]\u001b[A\n",
      "Train step of epoch 0:  46%|████▋     | 116/250 [01:43<01:58,  1.13it/s, loss=0.677]\u001b[A\n",
      "Train step of epoch 0:  46%|████▋     | 116/250 [01:43<01:58,  1.13it/s, loss=0.613]\u001b[A\n",
      "Train step of epoch 0:  47%|████▋     | 117/250 [01:44<01:57,  1.13it/s, loss=0.613]\u001b[A\n",
      "Train step of epoch 0:  47%|████▋     | 117/250 [01:44<01:57,  1.13it/s, loss=0.53] \u001b[A\n",
      "Train step of epoch 0:  47%|████▋     | 118/250 [01:45<01:56,  1.13it/s, loss=0.53]\u001b[A\n",
      "Train step of epoch 0:  47%|████▋     | 118/250 [01:45<01:56,  1.13it/s, loss=0.488]\u001b[A\n",
      "Train step of epoch 0:  48%|████▊     | 119/250 [01:46<01:56,  1.13it/s, loss=0.488]\u001b[A\n",
      "Train step of epoch 0:  48%|████▊     | 119/250 [01:46<01:56,  1.13it/s, loss=0.508]\u001b[A\n",
      "Train step of epoch 0:  48%|████▊     | 120/250 [01:46<01:55,  1.13it/s, loss=0.508]\u001b[A\n",
      "Train step of epoch 0:  48%|████▊     | 120/250 [01:47<01:55,  1.13it/s, loss=1.09] \u001b[A\n",
      "Train step of epoch 0:  48%|████▊     | 121/250 [01:47<01:54,  1.13it/s, loss=1.09]\u001b[A\n",
      "Train step of epoch 0:  48%|████▊     | 121/250 [01:47<01:54,  1.13it/s, loss=0.766]\u001b[A\n",
      "Train step of epoch 0:  49%|████▉     | 122/250 [01:48<01:53,  1.13it/s, loss=0.766]\u001b[A\n",
      "Train step of epoch 0:  49%|████▉     | 122/250 [01:48<01:53,  1.13it/s, loss=0.695]\u001b[A\n",
      "Train step of epoch 0:  49%|████▉     | 123/250 [01:49<01:52,  1.13it/s, loss=0.695]\u001b[A\n",
      "Train step of epoch 0:  49%|████▉     | 123/250 [01:49<01:52,  1.13it/s, loss=0.532]\u001b[A\n",
      "Train step of epoch 0:  50%|████▉     | 124/250 [01:50<01:51,  1.13it/s, loss=0.532]\u001b[A\n",
      "Train step of epoch 0:  50%|████▉     | 124/250 [01:50<01:51,  1.13it/s, loss=0.719]\u001b[A\n",
      "Train step of epoch 0:  50%|█████     | 125/250 [01:51<01:50,  1.13it/s, loss=0.719]\u001b[A\n",
      "Train step of epoch 0:  50%|█████     | 125/250 [01:51<01:50,  1.13it/s, loss=0.634]\u001b[A\n",
      "Train step of epoch 0:  50%|█████     | 126/250 [01:52<01:50,  1.13it/s, loss=0.634]\u001b[A\n",
      "Train step of epoch 0:  50%|█████     | 126/250 [01:52<01:50,  1.13it/s, loss=0.702]\u001b[A\n",
      "Train step of epoch 0:  51%|█████     | 127/250 [01:53<01:49,  1.13it/s, loss=0.702]\u001b[A\n",
      "Train step of epoch 0:  51%|█████     | 127/250 [01:53<01:49,  1.13it/s, loss=0.57] \u001b[A\n",
      "Train step of epoch 0:  51%|█████     | 128/250 [01:54<01:48,  1.13it/s, loss=0.57]\u001b[A\n",
      "Train step of epoch 0:  51%|█████     | 128/250 [01:54<01:48,  1.13it/s, loss=0.622]\u001b[A\n",
      "Train step of epoch 0:  52%|█████▏    | 129/250 [01:54<01:47,  1.12it/s, loss=0.622]\u001b[A\n",
      "Train step of epoch 0:  52%|█████▏    | 129/250 [01:55<01:47,  1.12it/s, loss=0.595]\u001b[A\n",
      "Train step of epoch 0:  52%|█████▏    | 130/250 [01:55<01:46,  1.12it/s, loss=0.595]\u001b[A\n",
      "Train step of epoch 0:  52%|█████▏    | 130/250 [01:55<01:46,  1.12it/s, loss=0.574]\u001b[A\n",
      "Train step of epoch 0:  52%|█████▏    | 131/250 [01:56<01:45,  1.12it/s, loss=0.574]\u001b[A\n",
      "Train step of epoch 0:  52%|█████▏    | 131/250 [01:56<01:45,  1.12it/s, loss=1]    \u001b[A\n",
      "Train step of epoch 0:  53%|█████▎    | 132/250 [01:57<01:45,  1.12it/s, loss=1]\u001b[A\n",
      "Train step of epoch 0:  53%|█████▎    | 132/250 [01:57<01:45,  1.12it/s, loss=0.649]\u001b[A\n",
      "Train step of epoch 0:  53%|█████▎    | 133/250 [01:58<01:44,  1.12it/s, loss=0.649]\u001b[A\n",
      "Train step of epoch 0:  53%|█████▎    | 133/250 [01:58<01:44,  1.12it/s, loss=0.788]\u001b[A\n",
      "Train step of epoch 0:  54%|█████▎    | 134/250 [01:59<01:43,  1.12it/s, loss=0.788]\u001b[A\n",
      "Train step of epoch 0:  54%|█████▎    | 134/250 [01:59<01:43,  1.12it/s, loss=0.766]\u001b[A\n",
      "Train step of epoch 0:  54%|█████▍    | 135/250 [02:00<01:42,  1.12it/s, loss=0.766]\u001b[A\n",
      "Train step of epoch 0:  54%|█████▍    | 135/250 [02:00<01:42,  1.12it/s, loss=0.689]\u001b[A\n",
      "Train step of epoch 0:  54%|█████▍    | 136/250 [02:01<01:41,  1.12it/s, loss=0.689]\u001b[A\n",
      "Train step of epoch 0:  54%|█████▍    | 136/250 [02:01<01:41,  1.12it/s, loss=0.723]\u001b[A\n",
      "Train step of epoch 0:  55%|█████▍    | 137/250 [02:02<01:40,  1.12it/s, loss=0.723]\u001b[A\n",
      "Train step of epoch 0:  55%|█████▍    | 137/250 [02:02<01:40,  1.12it/s, loss=0.511]\u001b[A\n",
      "Train step of epoch 0:  55%|█████▌    | 138/250 [02:03<01:40,  1.12it/s, loss=0.511]\u001b[A\n",
      "Train step of epoch 0:  55%|█████▌    | 138/250 [02:03<01:40,  1.12it/s, loss=0.612]\u001b[A\n",
      "Train step of epoch 0:  56%|█████▌    | 139/250 [02:03<01:39,  1.12it/s, loss=0.612]\u001b[A\n",
      "Train step of epoch 0:  56%|█████▌    | 139/250 [02:03<01:39,  1.12it/s, loss=0.503]\u001b[A\n",
      "Train step of epoch 0:  56%|█████▌    | 140/250 [02:04<01:38,  1.11it/s, loss=0.503]\u001b[A\n",
      "Train step of epoch 0:  56%|█████▌    | 140/250 [02:04<01:38,  1.11it/s, loss=0.5]  \u001b[A\n",
      "Train step of epoch 0:  56%|█████▋    | 141/250 [02:05<01:37,  1.11it/s, loss=0.5]\u001b[A\n",
      "Train step of epoch 0:  56%|█████▋    | 141/250 [02:05<01:37,  1.11it/s, loss=0.646]\u001b[A\n",
      "Train step of epoch 0:  57%|█████▋    | 142/250 [02:06<01:36,  1.11it/s, loss=0.646]\u001b[A\n",
      "Train step of epoch 0:  57%|█████▋    | 142/250 [02:06<01:36,  1.11it/s, loss=0.818]\u001b[A\n",
      "Train step of epoch 0:  57%|█████▋    | 143/250 [02:07<01:36,  1.11it/s, loss=0.818]\u001b[A\n",
      "Train step of epoch 0:  57%|█████▋    | 143/250 [02:07<01:36,  1.11it/s, loss=0.544]\u001b[A\n",
      "Train step of epoch 0:  58%|█████▊    | 144/250 [02:08<01:35,  1.11it/s, loss=0.544]\u001b[A\n",
      "Train step of epoch 0:  58%|█████▊    | 144/250 [02:08<01:35,  1.11it/s, loss=0.525]\u001b[A\n",
      "Train step of epoch 0:  58%|█████▊    | 145/250 [02:09<01:34,  1.11it/s, loss=0.525]\u001b[A\n",
      "Train step of epoch 0:  58%|█████▊    | 145/250 [02:09<01:34,  1.11it/s, loss=0.641]\u001b[A\n",
      "Train step of epoch 0:  58%|█████▊    | 146/250 [02:10<01:33,  1.11it/s, loss=0.641]\u001b[A\n",
      "Train step of epoch 0:  58%|█████▊    | 146/250 [02:10<01:33,  1.11it/s, loss=0.633]\u001b[A\n",
      "Train step of epoch 0:  59%|█████▉    | 147/250 [02:11<01:32,  1.11it/s, loss=0.633]\u001b[A\n",
      "Train step of epoch 0:  59%|█████▉    | 147/250 [02:11<01:32,  1.11it/s, loss=0.391]\u001b[A\n",
      "Train step of epoch 0:  59%|█████▉    | 148/250 [02:12<01:31,  1.11it/s, loss=0.391]\u001b[A\n",
      "Train step of epoch 0:  59%|█████▉    | 148/250 [02:12<01:31,  1.11it/s, loss=0.64] \u001b[A\n",
      "Train step of epoch 0:  60%|█████▉    | 149/250 [02:12<01:30,  1.11it/s, loss=0.64]\u001b[A\n",
      "Train step of epoch 0:  60%|█████▉    | 149/250 [02:12<01:30,  1.11it/s, loss=0.659]\u001b[A\n",
      "Train step of epoch 0:  60%|██████    | 150/250 [02:13<01:30,  1.11it/s, loss=0.659]\u001b[A\n",
      "Train step of epoch 0:  60%|██████    | 150/250 [02:13<01:30,  1.11it/s, loss=0.589]\u001b[A\n",
      "Train step of epoch 0:  60%|██████    | 151/250 [02:14<01:28,  1.11it/s, loss=0.589]\u001b[A\n",
      "Train step of epoch 0:  60%|██████    | 151/250 [02:14<01:28,  1.11it/s, loss=0.281]\u001b[A\n",
      "Train step of epoch 0:  61%|██████    | 152/250 [02:15<01:28,  1.11it/s, loss=0.281]\u001b[A\n",
      "Train step of epoch 0:  61%|██████    | 152/250 [02:15<01:28,  1.11it/s, loss=0.485]\u001b[A\n",
      "Train step of epoch 0:  61%|██████    | 153/250 [02:16<01:27,  1.11it/s, loss=0.485]\u001b[A\n",
      "Train step of epoch 0:  61%|██████    | 153/250 [02:16<01:27,  1.11it/s, loss=0.624]\u001b[A\n",
      "Train step of epoch 0:  62%|██████▏   | 154/250 [02:17<01:26,  1.11it/s, loss=0.624]\u001b[A\n",
      "Train step of epoch 0:  62%|██████▏   | 154/250 [02:17<01:26,  1.11it/s, loss=1.05] \u001b[A\n",
      "Train step of epoch 0:  62%|██████▏   | 155/250 [02:18<01:25,  1.11it/s, loss=1.05]\u001b[A\n",
      "Train step of epoch 0:  62%|██████▏   | 155/250 [02:18<01:25,  1.11it/s, loss=1]   \u001b[A\n",
      "Train step of epoch 0:  62%|██████▏   | 156/250 [02:19<01:24,  1.11it/s, loss=1]\u001b[A\n",
      "Train step of epoch 0:  62%|██████▏   | 156/250 [02:19<01:24,  1.11it/s, loss=0.504]\u001b[A\n",
      "Train step of epoch 0:  63%|██████▎   | 157/250 [02:20<01:23,  1.11it/s, loss=0.504]\u001b[A\n",
      "Train step of epoch 0:  63%|██████▎   | 157/250 [02:20<01:23,  1.11it/s, loss=0.515]\u001b[A\n",
      "Train step of epoch 0:  63%|██████▎   | 158/250 [02:21<01:22,  1.11it/s, loss=0.515]\u001b[A\n",
      "Train step of epoch 0:  63%|██████▎   | 158/250 [02:21<01:22,  1.11it/s, loss=0.739]\u001b[A\n",
      "Train step of epoch 0:  64%|██████▎   | 159/250 [02:21<01:21,  1.11it/s, loss=0.739]\u001b[A\n",
      "Train step of epoch 0:  64%|██████▎   | 159/250 [02:21<01:21,  1.11it/s, loss=0.712]\u001b[A\n",
      "Train step of epoch 0:  64%|██████▍   | 160/250 [02:22<01:20,  1.11it/s, loss=0.712]\u001b[A\n",
      "Train step of epoch 0:  64%|██████▍   | 160/250 [02:22<01:20,  1.11it/s, loss=0.452]\u001b[A\n",
      "Train step of epoch 0:  64%|██████▍   | 161/250 [02:23<01:19,  1.11it/s, loss=0.452]\u001b[A\n",
      "Train step of epoch 0:  64%|██████▍   | 161/250 [02:23<01:19,  1.11it/s, loss=0.693]\u001b[A\n",
      "Train step of epoch 0:  65%|██████▍   | 162/250 [02:24<01:19,  1.11it/s, loss=0.693]\u001b[A\n",
      "Train step of epoch 0:  65%|██████▍   | 162/250 [02:24<01:19,  1.11it/s, loss=0.535]\u001b[A\n",
      "Train step of epoch 0:  65%|██████▌   | 163/250 [02:25<01:18,  1.11it/s, loss=0.535]\u001b[A\n",
      "Train step of epoch 0:  65%|██████▌   | 163/250 [02:25<01:18,  1.11it/s, loss=0.425]\u001b[A\n",
      "Train step of epoch 0:  66%|██████▌   | 164/250 [02:26<01:17,  1.11it/s, loss=0.425]\u001b[A\n",
      "Train step of epoch 0:  66%|██████▌   | 164/250 [02:26<01:17,  1.11it/s, loss=0.621]\u001b[A\n",
      "Train step of epoch 0:  66%|██████▌   | 165/250 [02:27<01:16,  1.11it/s, loss=0.621]\u001b[A\n",
      "Train step of epoch 0:  66%|██████▌   | 165/250 [02:27<01:16,  1.11it/s, loss=1.02] \u001b[A\n",
      "Train step of epoch 0:  66%|██████▋   | 166/250 [02:28<01:15,  1.12it/s, loss=1.02]\u001b[A\n",
      "Train step of epoch 0:  66%|██████▋   | 166/250 [02:28<01:15,  1.12it/s, loss=0.304]\u001b[A\n",
      "Train step of epoch 0:  67%|██████▋   | 167/250 [02:29<01:14,  1.12it/s, loss=0.304]\u001b[A\n",
      "Train step of epoch 0:  67%|██████▋   | 167/250 [02:29<01:14,  1.12it/s, loss=0.427]\u001b[A\n",
      "Train step of epoch 0:  67%|██████▋   | 168/250 [02:29<01:13,  1.12it/s, loss=0.427]\u001b[A\n",
      "Train step of epoch 0:  67%|██████▋   | 168/250 [02:30<01:13,  1.12it/s, loss=0.47] \u001b[A\n",
      "Train step of epoch 0:  68%|██████▊   | 169/250 [02:30<01:12,  1.12it/s, loss=0.47]\u001b[A\n",
      "Train step of epoch 0:  68%|██████▊   | 169/250 [02:30<01:12,  1.12it/s, loss=0.412]\u001b[A\n",
      "Train step of epoch 0:  68%|██████▊   | 170/250 [02:31<01:11,  1.12it/s, loss=0.412]\u001b[A\n",
      "Train step of epoch 0:  68%|██████▊   | 170/250 [02:31<01:11,  1.12it/s, loss=0.811]\u001b[A\n",
      "Train step of epoch 0:  68%|██████▊   | 171/250 [02:32<01:10,  1.12it/s, loss=0.811]\u001b[A\n",
      "Train step of epoch 0:  68%|██████▊   | 171/250 [02:32<01:10,  1.12it/s, loss=0.679]\u001b[A\n",
      "Train step of epoch 0:  69%|██████▉   | 172/250 [02:33<01:09,  1.12it/s, loss=0.679]\u001b[A\n",
      "Train step of epoch 0:  69%|██████▉   | 172/250 [02:33<01:09,  1.12it/s, loss=0.821]\u001b[A\n",
      "Train step of epoch 0:  69%|██████▉   | 173/250 [02:34<01:08,  1.12it/s, loss=0.821]\u001b[A\n",
      "Train step of epoch 0:  69%|██████▉   | 173/250 [02:34<01:08,  1.12it/s, loss=0.402]\u001b[A\n",
      "Train step of epoch 0:  70%|██████▉   | 174/250 [02:35<01:07,  1.12it/s, loss=0.402]\u001b[A\n",
      "Train step of epoch 0:  70%|██████▉   | 174/250 [02:35<01:07,  1.12it/s, loss=0.33] \u001b[A\n",
      "Train step of epoch 0:  70%|███████   | 175/250 [02:36<01:06,  1.12it/s, loss=0.33]\u001b[A\n",
      "Train step of epoch 0:  70%|███████   | 175/250 [02:36<01:06,  1.12it/s, loss=0.606]\u001b[A\n",
      "Train step of epoch 0:  70%|███████   | 176/250 [02:37<01:06,  1.12it/s, loss=0.606]\u001b[A\n",
      "Train step of epoch 0:  70%|███████   | 176/250 [02:37<01:06,  1.12it/s, loss=0.454]\u001b[A\n",
      "Train step of epoch 0:  71%|███████   | 177/250 [02:38<01:05,  1.12it/s, loss=0.454]\u001b[A\n",
      "Train step of epoch 0:  71%|███████   | 177/250 [02:38<01:05,  1.12it/s, loss=0.383]\u001b[A\n",
      "Train step of epoch 0:  71%|███████   | 178/250 [02:38<01:04,  1.12it/s, loss=0.383]\u001b[A\n",
      "Train step of epoch 0:  71%|███████   | 178/250 [02:38<01:04,  1.12it/s, loss=0.366]\u001b[A\n",
      "Train step of epoch 0:  72%|███████▏  | 179/250 [02:39<01:03,  1.12it/s, loss=0.366]\u001b[A\n",
      "Train step of epoch 0:  72%|███████▏  | 179/250 [02:39<01:03,  1.12it/s, loss=0.545]\u001b[A\n",
      "Train step of epoch 0:  72%|███████▏  | 180/250 [02:40<01:02,  1.12it/s, loss=0.545]\u001b[A\n",
      "Train step of epoch 0:  72%|███████▏  | 180/250 [02:40<01:02,  1.12it/s, loss=0.64] \u001b[A\n",
      "Train step of epoch 0:  72%|███████▏  | 181/250 [02:41<01:01,  1.12it/s, loss=0.64]\u001b[A\n",
      "Train step of epoch 0:  72%|███████▏  | 181/250 [02:41<01:01,  1.12it/s, loss=0.307]\u001b[A\n",
      "Train step of epoch 0:  73%|███████▎  | 182/250 [02:42<01:00,  1.12it/s, loss=0.307]\u001b[A\n",
      "Train step of epoch 0:  73%|███████▎  | 182/250 [02:42<01:00,  1.12it/s, loss=1.03] \u001b[A\n",
      "Train step of epoch 0:  73%|███████▎  | 183/250 [02:43<00:59,  1.12it/s, loss=1.03]\u001b[A\n",
      "Train step of epoch 0:  73%|███████▎  | 183/250 [02:43<00:59,  1.12it/s, loss=1.26]\u001b[A\n",
      "Train step of epoch 0:  74%|███████▎  | 184/250 [02:44<00:58,  1.12it/s, loss=1.26]\u001b[A\n",
      "Train step of epoch 0:  74%|███████▎  | 184/250 [02:44<00:58,  1.12it/s, loss=0.917]\u001b[A\n",
      "Train step of epoch 0:  74%|███████▍  | 185/250 [02:45<00:57,  1.12it/s, loss=0.917]\u001b[A\n",
      "Train step of epoch 0:  74%|███████▍  | 185/250 [02:45<00:57,  1.12it/s, loss=0.633]\u001b[A\n",
      "Train step of epoch 0:  74%|███████▍  | 186/250 [02:46<00:57,  1.12it/s, loss=0.633]\u001b[A\n",
      "Train step of epoch 0:  74%|███████▍  | 186/250 [02:46<00:57,  1.12it/s, loss=0.362]\u001b[A\n",
      "Train step of epoch 0:  75%|███████▍  | 187/250 [02:46<00:56,  1.12it/s, loss=0.362]\u001b[A\n",
      "Train step of epoch 0:  75%|███████▍  | 187/250 [02:46<00:56,  1.12it/s, loss=0.364]\u001b[A\n",
      "Train step of epoch 0:  75%|███████▌  | 188/250 [02:47<00:55,  1.12it/s, loss=0.364]\u001b[A\n",
      "Train step of epoch 0:  75%|███████▌  | 188/250 [02:47<00:55,  1.12it/s, loss=0.682]\u001b[A\n",
      "Train step of epoch 0:  76%|███████▌  | 189/250 [02:48<00:54,  1.12it/s, loss=0.682]\u001b[A\n",
      "Train step of epoch 0:  76%|███████▌  | 189/250 [02:48<00:54,  1.12it/s, loss=0.826]\u001b[A\n",
      "Train step of epoch 0:  76%|███████▌  | 190/250 [02:49<00:53,  1.12it/s, loss=0.826]\u001b[A\n",
      "Train step of epoch 0:  76%|███████▌  | 190/250 [02:49<00:53,  1.12it/s, loss=0.426]\u001b[A\n",
      "Train step of epoch 0:  76%|███████▋  | 191/250 [02:50<00:52,  1.12it/s, loss=0.426]\u001b[A\n",
      "Train step of epoch 0:  76%|███████▋  | 191/250 [02:50<00:52,  1.12it/s, loss=0.348]\u001b[A\n",
      "Train step of epoch 0:  77%|███████▋  | 192/250 [02:51<00:51,  1.12it/s, loss=0.348]\u001b[A\n",
      "Train step of epoch 0:  77%|███████▋  | 192/250 [02:51<00:51,  1.12it/s, loss=0.44] \u001b[A\n",
      "Train step of epoch 0:  77%|███████▋  | 193/250 [02:52<00:50,  1.12it/s, loss=0.44]\u001b[A\n",
      "Train step of epoch 0:  77%|███████▋  | 193/250 [02:52<00:50,  1.12it/s, loss=0.902]\u001b[A\n",
      "Train step of epoch 0:  78%|███████▊  | 194/250 [02:53<00:49,  1.12it/s, loss=0.902]\u001b[A\n",
      "Train step of epoch 0:  78%|███████▊  | 194/250 [02:53<00:49,  1.12it/s, loss=0.708]\u001b[A\n",
      "Train step of epoch 0:  78%|███████▊  | 195/250 [02:54<00:48,  1.12it/s, loss=0.708]\u001b[A\n",
      "Train step of epoch 0:  78%|███████▊  | 195/250 [02:54<00:48,  1.12it/s, loss=0.399]\u001b[A\n",
      "Train step of epoch 0:  78%|███████▊  | 196/250 [02:54<00:48,  1.12it/s, loss=0.399]\u001b[A\n",
      "Train step of epoch 0:  78%|███████▊  | 196/250 [02:54<00:48,  1.12it/s, loss=1.03] \u001b[A\n",
      "Train step of epoch 0:  79%|███████▉  | 197/250 [02:55<00:47,  1.12it/s, loss=1.03]\u001b[A\n",
      "Train step of epoch 0:  79%|███████▉  | 197/250 [02:55<00:47,  1.12it/s, loss=0.649]\u001b[A\n",
      "Train step of epoch 0:  79%|███████▉  | 198/250 [02:56<00:46,  1.12it/s, loss=0.649]\u001b[A\n",
      "Train step of epoch 0:  79%|███████▉  | 198/250 [02:56<00:46,  1.12it/s, loss=0.491]\u001b[A\n",
      "Train step of epoch 0:  80%|███████▉  | 199/250 [02:57<00:45,  1.12it/s, loss=0.491]\u001b[A\n",
      "Train step of epoch 0:  80%|███████▉  | 199/250 [02:57<00:45,  1.12it/s, loss=0.461]\u001b[A\n",
      "Train step of epoch 0:  80%|████████  | 200/250 [02:58<00:44,  1.12it/s, loss=0.461]\u001b[A\n",
      "Train step of epoch 0:  80%|████████  | 200/250 [02:58<00:44,  1.12it/s, loss=0.48] \u001b[A\n",
      "Train step of epoch 0:  80%|████████  | 201/250 [02:59<00:43,  1.12it/s, loss=0.48]\u001b[A\n",
      "Train step of epoch 0:  80%|████████  | 201/250 [02:59<00:43,  1.12it/s, loss=0.482]\u001b[A\n",
      "Train step of epoch 0:  81%|████████  | 202/250 [03:00<00:42,  1.12it/s, loss=0.482]\u001b[A\n",
      "Train step of epoch 0:  81%|████████  | 202/250 [03:00<00:42,  1.12it/s, loss=0.758]\u001b[A\n",
      "Train step of epoch 0:  81%|████████  | 203/250 [03:01<00:41,  1.12it/s, loss=0.758]\u001b[A\n",
      "Train step of epoch 0:  81%|████████  | 203/250 [03:01<00:41,  1.12it/s, loss=0.636]\u001b[A\n",
      "Train step of epoch 0:  82%|████████▏ | 204/250 [03:02<00:40,  1.12it/s, loss=0.636]\u001b[A\n",
      "Train step of epoch 0:  82%|████████▏ | 204/250 [03:02<00:40,  1.12it/s, loss=0.53] \u001b[A\n",
      "Train step of epoch 0:  82%|████████▏ | 205/250 [03:02<00:40,  1.12it/s, loss=0.53]\u001b[A\n",
      "Train step of epoch 0:  82%|████████▏ | 205/250 [03:02<00:40,  1.12it/s, loss=0.582]\u001b[A\n",
      "Train step of epoch 0:  82%|████████▏ | 206/250 [03:03<00:39,  1.12it/s, loss=0.582]\u001b[A\n",
      "Train step of epoch 0:  82%|████████▏ | 206/250 [03:03<00:39,  1.12it/s, loss=0.762]\u001b[A\n",
      "Train step of epoch 0:  83%|████████▎ | 207/250 [03:04<00:38,  1.12it/s, loss=0.762]\u001b[A\n",
      "Train step of epoch 0:  83%|████████▎ | 207/250 [03:04<00:38,  1.12it/s, loss=0.527]\u001b[A\n",
      "Train step of epoch 0:  83%|████████▎ | 208/250 [03:05<00:37,  1.12it/s, loss=0.527]\u001b[A\n",
      "Train step of epoch 0:  83%|████████▎ | 208/250 [03:05<00:37,  1.12it/s, loss=0.684]\u001b[A\n",
      "Train step of epoch 0:  84%|████████▎ | 209/250 [03:06<00:36,  1.12it/s, loss=0.684]\u001b[A\n",
      "Train step of epoch 0:  84%|████████▎ | 209/250 [03:06<00:36,  1.12it/s, loss=0.722]\u001b[A\n",
      "Train step of epoch 0:  84%|████████▍ | 210/250 [03:07<00:35,  1.12it/s, loss=0.722]\u001b[A\n",
      "Train step of epoch 0:  84%|████████▍ | 210/250 [03:07<00:35,  1.12it/s, loss=0.671]\u001b[A\n",
      "Train step of epoch 0:  84%|████████▍ | 211/250 [03:08<00:34,  1.12it/s, loss=0.671]\u001b[A\n",
      "Train step of epoch 0:  84%|████████▍ | 211/250 [03:08<00:34,  1.12it/s, loss=0.425]\u001b[A\n",
      "Train step of epoch 0:  85%|████████▍ | 212/250 [03:09<00:33,  1.12it/s, loss=0.425]\u001b[A\n",
      "Train step of epoch 0:  85%|████████▍ | 212/250 [03:09<00:33,  1.12it/s, loss=0.551]\u001b[A\n",
      "Train step of epoch 0:  85%|████████▌ | 213/250 [03:10<00:32,  1.12it/s, loss=0.551]\u001b[A\n",
      "Train step of epoch 0:  85%|████████▌ | 213/250 [03:10<00:32,  1.12it/s, loss=0.412]\u001b[A\n",
      "Train step of epoch 0:  86%|████████▌ | 214/250 [03:10<00:32,  1.12it/s, loss=0.412]\u001b[A\n",
      "Train step of epoch 0:  86%|████████▌ | 214/250 [03:11<00:32,  1.12it/s, loss=1.09] \u001b[A\n",
      "Train step of epoch 0:  86%|████████▌ | 215/250 [03:11<00:31,  1.12it/s, loss=1.09]\u001b[A\n",
      "Train step of epoch 0:  86%|████████▌ | 215/250 [03:11<00:31,  1.12it/s, loss=0.474]\u001b[A\n",
      "Train step of epoch 0:  86%|████████▋ | 216/250 [03:12<00:30,  1.12it/s, loss=0.474]\u001b[A\n",
      "Train step of epoch 0:  86%|████████▋ | 216/250 [03:12<00:30,  1.12it/s, loss=0.836]\u001b[A\n",
      "Train step of epoch 0:  87%|████████▋ | 217/250 [03:13<00:29,  1.12it/s, loss=0.836]\u001b[A\n",
      "Train step of epoch 0:  87%|████████▋ | 217/250 [03:13<00:29,  1.12it/s, loss=0.838]\u001b[A\n",
      "Train step of epoch 0:  87%|████████▋ | 218/250 [03:14<00:28,  1.12it/s, loss=0.838]\u001b[A\n",
      "Train step of epoch 0:  87%|████████▋ | 218/250 [03:14<00:28,  1.12it/s, loss=0.554]\u001b[A\n",
      "Train step of epoch 0:  88%|████████▊ | 219/250 [03:15<00:27,  1.12it/s, loss=0.554]\u001b[A\n",
      "Train step of epoch 0:  88%|████████▊ | 219/250 [03:15<00:27,  1.12it/s, loss=0.366]\u001b[A\n",
      "Train step of epoch 0:  88%|████████▊ | 220/250 [03:16<00:26,  1.12it/s, loss=0.366]\u001b[A\n",
      "Train step of epoch 0:  88%|████████▊ | 220/250 [03:16<00:26,  1.12it/s, loss=0.614]\u001b[A\n",
      "Train step of epoch 0:  88%|████████▊ | 221/250 [03:17<00:25,  1.12it/s, loss=0.614]\u001b[A\n",
      "Train step of epoch 0:  88%|████████▊ | 221/250 [03:17<00:25,  1.12it/s, loss=0.658]\u001b[A\n",
      "Train step of epoch 0:  89%|████████▉ | 222/250 [03:18<00:25,  1.12it/s, loss=0.658]\u001b[A\n",
      "Train step of epoch 0:  89%|████████▉ | 222/250 [03:18<00:25,  1.12it/s, loss=0.304]\u001b[A\n",
      "Train step of epoch 0:  89%|████████▉ | 223/250 [03:19<00:24,  1.12it/s, loss=0.304]\u001b[A\n",
      "Train step of epoch 0:  89%|████████▉ | 223/250 [03:19<00:24,  1.12it/s, loss=0.734]\u001b[A\n",
      "Train step of epoch 0:  90%|████████▉ | 224/250 [03:19<00:23,  1.12it/s, loss=0.734]\u001b[A\n",
      "Train step of epoch 0:  90%|████████▉ | 224/250 [03:19<00:23,  1.12it/s, loss=0.766]\u001b[A\n",
      "Train step of epoch 0:  90%|█████████ | 225/250 [03:20<00:22,  1.12it/s, loss=0.766]\u001b[A\n",
      "Train step of epoch 0:  90%|█████████ | 225/250 [03:20<00:22,  1.12it/s, loss=0.608]\u001b[A\n",
      "Train step of epoch 0:  90%|█████████ | 226/250 [03:21<00:21,  1.12it/s, loss=0.608]\u001b[A\n",
      "Train step of epoch 0:  90%|█████████ | 226/250 [03:21<00:21,  1.12it/s, loss=0.675]\u001b[A\n",
      "Train step of epoch 0:  91%|█████████ | 227/250 [03:22<00:20,  1.12it/s, loss=0.675]\u001b[A\n",
      "Train step of epoch 0:  91%|█████████ | 227/250 [03:22<00:20,  1.12it/s, loss=0.71] \u001b[A\n",
      "Train step of epoch 0:  91%|█████████ | 228/250 [03:23<00:19,  1.12it/s, loss=0.71]\u001b[A\n",
      "Train step of epoch 0:  91%|█████████ | 228/250 [03:23<00:19,  1.12it/s, loss=0.496]\u001b[A\n",
      "Train step of epoch 0:  92%|█████████▏| 229/250 [03:24<00:18,  1.12it/s, loss=0.496]\u001b[A\n",
      "Train step of epoch 0:  92%|█████████▏| 229/250 [03:24<00:18,  1.12it/s, loss=0.727]\u001b[A\n",
      "Train step of epoch 0:  92%|█████████▏| 230/250 [03:25<00:17,  1.12it/s, loss=0.727]\u001b[A\n",
      "Train step of epoch 0:  92%|█████████▏| 230/250 [03:25<00:17,  1.12it/s, loss=0.744]\u001b[A\n",
      "Train step of epoch 0:  92%|█████████▏| 231/250 [03:26<00:16,  1.12it/s, loss=0.744]\u001b[A\n",
      "Train step of epoch 0:  92%|█████████▏| 231/250 [03:26<00:16,  1.12it/s, loss=0.391]\u001b[A\n",
      "Train step of epoch 0:  93%|█████████▎| 232/250 [03:27<00:16,  1.12it/s, loss=0.391]\u001b[A\n",
      "Train step of epoch 0:  93%|█████████▎| 232/250 [03:27<00:16,  1.12it/s, loss=0.867]\u001b[A\n",
      "Train step of epoch 0:  93%|█████████▎| 233/250 [03:27<00:15,  1.12it/s, loss=0.867]\u001b[A\n",
      "Train step of epoch 0:  93%|█████████▎| 233/250 [03:28<00:15,  1.12it/s, loss=0.524]\u001b[A\n",
      "Train step of epoch 0:  94%|█████████▎| 234/250 [03:28<00:14,  1.12it/s, loss=0.524]\u001b[A\n",
      "Train step of epoch 0:  94%|█████████▎| 234/250 [03:28<00:14,  1.12it/s, loss=0.555]\u001b[A\n",
      "Train step of epoch 0:  94%|█████████▍| 235/250 [03:29<00:13,  1.12it/s, loss=0.555]\u001b[A\n",
      "Train step of epoch 0:  94%|█████████▍| 235/250 [03:29<00:13,  1.12it/s, loss=0.948]\u001b[A\n",
      "Train step of epoch 0:  94%|█████████▍| 236/250 [03:30<00:12,  1.12it/s, loss=0.948]\u001b[A\n",
      "Train step of epoch 0:  94%|█████████▍| 236/250 [03:30<00:12,  1.12it/s, loss=0.528]\u001b[A\n",
      "Train step of epoch 0:  95%|█████████▍| 237/250 [03:31<00:11,  1.12it/s, loss=0.528]\u001b[A\n",
      "Train step of epoch 0:  95%|█████████▍| 237/250 [03:31<00:11,  1.12it/s, loss=0.519]\u001b[A\n",
      "Train step of epoch 0:  95%|█████████▌| 238/250 [03:32<00:10,  1.12it/s, loss=0.519]\u001b[A\n",
      "Train step of epoch 0:  95%|█████████▌| 238/250 [03:32<00:10,  1.12it/s, loss=0.749]\u001b[A\n",
      "Train step of epoch 0:  96%|█████████▌| 239/250 [03:33<00:09,  1.12it/s, loss=0.749]\u001b[A\n",
      "Train step of epoch 0:  96%|█████████▌| 239/250 [03:33<00:09,  1.12it/s, loss=0.69] \u001b[A\n",
      "Train step of epoch 0:  96%|█████████▌| 240/250 [03:34<00:08,  1.12it/s, loss=0.69]\u001b[A\n",
      "Train step of epoch 0:  96%|█████████▌| 240/250 [03:34<00:08,  1.12it/s, loss=0.597]\u001b[A\n",
      "Train step of epoch 0:  96%|█████████▋| 241/250 [03:35<00:08,  1.12it/s, loss=0.597]\u001b[A\n",
      "Train step of epoch 0:  96%|█████████▋| 241/250 [03:35<00:08,  1.12it/s, loss=0.498]\u001b[A\n",
      "Train step of epoch 0:  97%|█████████▋| 242/250 [03:36<00:07,  1.12it/s, loss=0.498]\u001b[A\n",
      "Train step of epoch 0:  97%|█████████▋| 242/250 [03:36<00:07,  1.12it/s, loss=0.838]\u001b[A\n",
      "Train step of epoch 0:  97%|█████████▋| 243/250 [03:36<00:06,  1.12it/s, loss=0.838]\u001b[A\n",
      "Train step of epoch 0:  97%|█████████▋| 243/250 [03:36<00:06,  1.12it/s, loss=0.575]\u001b[A\n",
      "Train step of epoch 0:  98%|█████████▊| 244/250 [03:37<00:05,  1.12it/s, loss=0.575]\u001b[A\n",
      "Train step of epoch 0:  98%|█████████▊| 244/250 [03:37<00:05,  1.12it/s, loss=0.644]\u001b[A\n",
      "Train step of epoch 0:  98%|█████████▊| 245/250 [03:38<00:04,  1.12it/s, loss=0.644]\u001b[A\n",
      "Train step of epoch 0:  98%|█████████▊| 245/250 [03:38<00:04,  1.12it/s, loss=0.821]\u001b[A\n",
      "Train step of epoch 0:  98%|█████████▊| 246/250 [03:39<00:03,  1.12it/s, loss=0.821]\u001b[A\n",
      "Train step of epoch 0:  98%|█████████▊| 246/250 [03:39<00:03,  1.12it/s, loss=0.791]\u001b[A\n",
      "Train step of epoch 0:  99%|█████████▉| 247/250 [03:40<00:02,  1.12it/s, loss=0.791]\u001b[A\n",
      "Train step of epoch 0:  99%|█████████▉| 247/250 [03:40<00:02,  1.12it/s, loss=0.402]\u001b[A\n",
      "Train step of epoch 0:  99%|█████████▉| 248/250 [03:41<00:01,  1.12it/s, loss=0.402]\u001b[A\n",
      "Train step of epoch 0:  99%|█████████▉| 248/250 [03:41<00:01,  1.12it/s, loss=1.17] \u001b[A\n",
      "Train step of epoch 0: 100%|█████████▉| 249/250 [03:42<00:00,  1.12it/s, loss=1.17]\u001b[A\n",
      "Train step of epoch 0: 100%|█████████▉| 249/250 [03:42<00:00,  1.12it/s, loss=0.631]\u001b[A\n",
      "Train step of epoch 0: 100%|██████████| 250/250 [03:43<00:00,  1.12it/s, loss=0.631]\u001b[A\n",
      "Train epoch: 100%|██████████| 1/1 [03:57<00:00, 237.91s/it]0,  1.12it/s, loss=0.467]\u001b[A\n",
      "Train step of epoch 0: 100%|██████████| 250/250 [03:57<00:00,  1.05it/s, loss=0.617, dist_mean=0.289]\u001b[A\n",
      "Train epoch: 100%|██████████| 1/1 [03:57<00:00, 237.92s/it]\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(use_lora=0)\n",
    "\n",
    "model.save_pretrained('aiffel/KoChatGPT/output_2_RM')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bb31d8",
   "metadata": {},
   "source": [
    "- 임의 문장 입력 후 reward score 확인, RM 학습 확인 위하"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "be9951f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: 인공지능은 똥멍청이 입니다\n",
      "reward score: -2.2\n"
     ]
    }
   ],
   "source": [
    "def inference_RM(input_text):\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(\n",
    "        torch.cuda.current_device())\n",
    "    output = model(input_ids)\n",
    "    output_reward = output.cpu().detach().numpy()[0]\n",
    "\n",
    "    print('input: %s\\nreward score: %.1f'%(input_text, output_reward))\n",
    "\n",
    "    return output_reward\n",
    "\n",
    "input_text = '인공지능은 똥멍청이 입니다'\n",
    "output_reward = inference_RM(input_text=input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d3e1bd34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: 인공지능(AI)은 컴퓨터에서 음성 및 작성된 언어를 보고 이해하고 번역하고 데이터를 분석하고 추천하는 기능을 포함하여 다양한 고급 기능을 수행할 수 있는 일련의 기술입니다.\n",
      "reward score: -2.1\n"
     ]
    }
   ],
   "source": [
    "input_text = '인공지능(AI)은 컴퓨터에서 음성 및 작성된 언어를 보고 이해하고 번역하고 데이터를 분석하고 추천하는 기능을 포함하여 다양한 고급 기능을 수행할 수 있는 일련의 기술입니다.'\n",
    "\n",
    "output_reward = inference_RM(input_text=input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "90ac5e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: 인공지능(AI)은 컴퓨터에서 음성 및 작성된 언어를 보고 이해하고 번역하고 데이터를 분석하고 추천하는 기능을 포함하여 다양한 고급 기능을 수행할 수 있는 일련의 기술입니다. AI는 현대적인 컴퓨팅 혁신에서 중추적인 역할을 하며 개인과 비즈니스의 가치를 창출합니다. 예를 들어 광학 문자 인식(OCR)은 AI를 사용해 이미지 및 문서에서 텍스트 및 데이터를 추출하고, 구조화되지 않은 콘텐츠를 비즈니스에 바로 사용할 수 있게 만들고, 유용한 정보를 창출합니다.\n",
      "reward score: -2.0\n"
     ]
    }
   ],
   "source": [
    "input_text = \"인공지능(AI)은 컴퓨터에서 음성 및 작성된 언어를 보고 이해하고 번역하고 데이터를 분석하고 추천하는 기능을 포함하여 다양한 고급 기능을 수행할 수 있는 일련의 기술입니다. AI는 현대적인 컴퓨팅 혁신에서 중추적인 역할을 하며 개인과 비즈니스의 가치를 창출합니다. 예를 들어 광학 문자 인식(OCR)은 AI를 사용해 이미지 및 문서에서 텍스트 및 데이터를 추출하고, 구조화되지 않은 콘텐츠를 비즈니스에 바로 사용할 수 있게 만들고, 유용한 정보를 창출합니다.\"\n",
    "\n",
    "output_reward = inference_RM(input_text=input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4d634128",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: 인공지능은 일반적으로 인간의 지능이 필요하거나 인간이 분석할 수 있는 것보다 규모가 큰 데이터를 포함하는 방식으로 추론, 학습 및 행동할 수 있는 컴퓨터 및 기계를 구축하는 것과 관련된 과학 분야입니다. AI는 컴퓨터 공학, 데이터 분석 및 통계, 하드웨어 및 소프트웨어 엔지니어링, 언어학, 신경 과학은 물론 철학과 심리학을 포함하여 여러 학문을 포괄하는 광범위한 분야입니다. 비즈니스의 운영 수준에서 AI는 주로 머신러닝과 딥 러닝을 기반으로 하는 기술 모음으로, 데이터 분석, 예상 및 예측, 객체 분류, 자연어 처리, 추천, 지능형 데이터 가져오기 등을 수행할 수 있습니다.\n",
      "reward score: -1.9\n"
     ]
    }
   ],
   "source": [
    "input_text = \"인공지능은 일반적으로 인간의 지능이 필요하거나 인간이 분석할 수 있는 것보다 규모가 큰 데이터를 포함하는 방식으로 추론, 학습 및 행동할 수 있는 컴퓨터 및 기계를 구축하는 것과 관련된 과학 분야입니다. AI는 컴퓨터 공학, 데이터 분석 및 통계, 하드웨어 및 소프트웨어 엔지니어링, 언어학, 신경 과학은 물론 철학과 심리학을 포함하여 여러 학문을 포괄하는 광범위한 분야입니다. 비즈니스의 운영 수준에서 AI는 주로 머신러닝과 딥 러닝을 기반으로 하는 기술 모음으로, 데이터 분석, 예상 및 예측, 객체 분류, 자연어 처리, 추천, 지능형 데이터 가져오기 등을 수행할 수 있습니다.\"\n",
    "\n",
    "output_reward = inference_RM(input_text=input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "acdff919",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_prompt = ['불고기용 고기 한우에요?',\n",
    "               '리처드 닉슨이 43대 부통령직을 수행한 년도는?',\n",
    "               '시카고 오헤어 국제공항은 어디에 있어?',\n",
    "               '오늘 미세먼지 어때?',\n",
    "              '쓰던 앱이 유료로 전환됐어',\n",
    "              '여친이랑 다툼']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "141783f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: 불고기용 고기 한우에요?\n",
      "reward score: -2.5\n",
      "-2.511071\n",
      "input: 리처드 닉슨이 43대 부통령직을 수행한 년도는?\n",
      "reward score: -2.3\n",
      "-2.3143651\n",
      "input: 시카고 오헤어 국제공항은 어디에 있어?\n",
      "reward score: -2.3\n",
      "-2.3128173\n",
      "input: 오늘 미세먼지 어때?\n",
      "reward score: -2.5\n",
      "-2.5138245\n",
      "input: 쓰던 앱이 유료로 전환됐어\n",
      "reward score: -2.4\n",
      "-2.383667\n",
      "input: 여친이랑 다툼\n",
      "reward score: -2.5\n",
      "-2.4653347\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(list_prompt)):\n",
    "    input_text = list_prompt[i]\n",
    "    output_reward = inference_RM(input_text=input_text)\n",
    "    print(output_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1feecff4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: 안녕하세요, 저는 이 고기를 구매하려 하는데요. 혹시 이 고기가 불고기용 한우인지 확인 부탁드립니다.\n",
      "reward score: -2.5\n"
     ]
    }
   ],
   "source": [
    "input_text = \"안녕하세요, 저는 이 고기를 구매하려 하는데요. 혹시 이 고기가 불고기용 한우인지 확인 부탁드립니다.\"\n",
    "\n",
    "\n",
    "\n",
    "output_reward = inference_RM(input_text=input_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64931f01",
   "metadata": {},
   "source": [
    "#### RM과 SFT의 주요 차이점\n",
    "- 목적: SFT는 특정 작업(예: 특정 스타일의 텍스트 생성)에 대한 모델의 성능을 직접적으로 향상시키는 데 중점을 둡니다. 반면, RM은 생성된 텍스트의 품질을 평가하는 보상 함수를 학습하고, 이를 사용하여 보다 질 높은 텍스트를 생성하도록 모델을 유도합니다.\n",
    "- 모델 구조: SFT는 기본 모델에 대한 수정이 거의 없거나 전혀 없이 진행될 수 있습니다. RM 구현에서는 보상을 계산하기 위해 추가적인 \"value head\"를 모델에 도입하는 경우가 많습니다. 이러한 차이는 모델이 어떻게 확장되고 조정되는지에 영향을 미칩니다.\n",
    "- 학습 데이터: SFT는 주로 특정 작업에 적합한 라벨이 붙은 데이터셋을 사용합니다. RM은 사용자의 선호도, 품질 등에 따라 평가된 데이터셋(예: 텍스트 쌍과 이에 대한 선호도 점수)이 필요합니다.\n",
    "- 학습 방식: RM은 보상을 최대화하는 방향으로 모델을 학습시키는 반면, SFT는 주어진 태스크에 대한 모델의 성능을 직접 최적화합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28121f6",
   "metadata": {},
   "source": [
    "## Finetuning GPT-2 with LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42088293",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c93547ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-04-02 00:02:05--  https://github.com/microsoft/LoRA/raw/main/examples/NLG/data/e2e/train.txt\n",
      "Resolving github.com (github.com)... 140.82.113.4\n",
      "Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/microsoft/LoRA/main/examples/NLG/data/e2e/train.txt [following]\n",
      "--2024-04-02 00:02:06--  https://raw.githubusercontent.com/microsoft/LoRA/main/examples/NLG/data/e2e/train.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 9624463 (9.2M) [text/plain]\n",
      "Saving to: ‘train.txt’\n",
      "\n",
      "train.txt           100%[===================>]   9.18M  --.-KB/s    in 0.07s   \n",
      "\n",
      "2024-04-02 00:02:06 (128 MB/s) - ‘train.txt’ saved [9624463/9624463]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/microsoft/LoRA/raw/main/examples/NLG/data/e2e/train.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "794142ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-04-02 00:02:12--  https://github.com/microsoft/LoRA/raw/main/examples/NLG/data/e2e/test.txt\n",
      "Resolving github.com (github.com)... 140.82.112.4\n",
      "Connecting to github.com (github.com)|140.82.112.4|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/microsoft/LoRA/main/examples/NLG/data/e2e/test.txt [following]\n",
      "--2024-04-02 00:02:12--  https://raw.githubusercontent.com/microsoft/LoRA/main/examples/NLG/data/e2e/test.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1351149 (1.3M) [text/plain]\n",
      "Saving to: ‘test.txt’\n",
      "\n",
      "test.txt            100%[===================>]   1.29M  --.-KB/s    in 0.04s   \n",
      "\n",
      "2024-04-02 00:02:13 (34.8 MB/s) - ‘test.txt’ saved [1351149/1351149]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/microsoft/LoRA/raw/main/examples/NLG/data/e2e/test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8655465e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name : The Vaults | Type : pub | price : more than £ 30 | customer rating : 5 out of 5 | near : Café Adriatic||The Vaults pub near Café Adriatic has a 5 star rating . Prices start at £ 30 . \r\n",
      "name : The Cambridge Blue | Type : pub | food : English | price : cheap | near : Café Brazil||Close to Café Brazil , The Cambridge Blue pub serves delicious Tuscan Beef for the cheap price of £ 10.50 . Delicious Pub food . \r\n",
      "name : The Eagle | Type : coffee shop | food : Japanese | price : less than £ 20 | customer rating : low | area : riverside | family friendly : yes | near : Burger King||The Eagle is a low rated coffee shop near Burger King and the riverside that is family friendly and is less than £ 20 for Japanese food . \r\n",
      "name : The Mill | Type : coffee shop | food : French | price : £ 20 - 25 | area : riverside | near : The Sorrento||Located near The Sorrento is a French Theme eatery and coffee shop called The Mill , with a price range at £ 20- £ 25 it is in the riverside area . \r\n",
      "name : Loch Fyne | food : French | customer rating : high | area : riverside | near : The Rice Boat||For luxurious French food , the Loch Fyne is located by the river next to The Rice Boat . \r\n"
     ]
    }
   ],
   "source": [
    "!head -n 5 train.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fa99cb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import io\n",
    "import json\n",
    "\n",
    "def format_convert(read_file, write_file):\n",
    "    with open(read_file, \"r\", encoding=\"utf8\") as reader, \\\n",
    "    \t open(write_file, \"w\", encoding=\"utf8\") as writer :\n",
    "    \tfor line in reader:\n",
    "    \t\titems = line.strip().split(\"||\")\n",
    "    \t\tcontext = items[0]\n",
    "    \t\tcompletion = items[1].strip(\"\\n\")\n",
    "    \t\tx = {}\n",
    "    \t\tx[\"context\"] = context\n",
    "    \t\tx[\"completion\"] = completion\n",
    "    \t\twriter.write(json.dumps(x)+\"\\n\")\n",
    "\n",
    "format_convert(\"train.txt\", \"train_formatted.jsonl\")\n",
    "format_convert(\"test.txt\", \"test_formatted.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7ac4ed77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"context\": \"name : The Vaults | Type : pub | price : more than \\u00a3 30 | customer rating : 5 out of 5 | near : Caf\\u00e9 Adriatic\", \"completion\": \"The Vaults pub near Caf\\u00e9 Adriatic has a 5 star rating . Prices start at \\u00a3 30 .\"}\n",
      "\n",
      "{\"context\": \"name : The Cambridge Blue | Type : pub | food : English | price : cheap | near : Caf\\u00e9 Brazil\", \"completion\": \"Close to Caf\\u00e9 Brazil , The Cambridge Blue pub serves delicious Tuscan Beef for the cheap price of \\u00a3 10.50 . Delicious Pub food .\"}\n",
      "\n",
      "{\"context\": \"name : The Eagle | Type : coffee shop | food : Japanese | price : less than \\u00a3 20 | customer rating : low | area : riverside | family friendly : yes | near : Burger King\", \"completion\": \"The Eagle is a low rated coffee shop near Burger King and the riverside that is family friendly and is less than \\u00a3 20 for Japanese food .\"}\n",
      "\n",
      "{\"context\": \"name : The Mill | Type : coffee shop | food : French | price : \\u00a3 20 - 25 | area : riverside | near : The Sorrento\", \"completion\": \"Located near The Sorrento is a French Theme eatery and coffee shop called The Mill , with a price range at \\u00a3 20- \\u00a3 25 it is in the riverside area .\"}\n",
      "\n",
      "{\"context\": \"name : Loch Fyne | food : French | customer rating : high | area : riverside | near : The Rice Boat\", \"completion\": \"For luxurious French food , the Loch Fyne is located by the river next to The Rice Boat .\"}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"train_formatted.jsonl\", \"r\") as reader:\n",
    "    for _ in range(5):\n",
    "        print(next(reader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f09e99f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import os\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    fast_tokenizer=True)\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "913c372b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length of tokens is 1000000000000000019884624838656 in this model.\n",
      "But here we use max 512 tokens in the training.\n"
     ]
    }
   ],
   "source": [
    "block_size = 512\n",
    "\n",
    "print(f\"Max length of tokens is {tokenizer.model_max_length} in this model.\")\n",
    "print(f\"But here we use max {block_size} tokens in the training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "653bf3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "\n",
    "def fill_ignore_label(l, c):\n",
    "    l[:len(c) - 1] = [-100] * (len(c) - 1)\n",
    "    return l\n",
    "\n",
    "def pad_tokens(tokens, max_seq_length, padding_token):\n",
    "    res_tokens = tokens[:max_seq_length]\n",
    "    token_len = len(res_tokens)\n",
    "    res_tokens = res_tokens + \\\n",
    "        [padding_token for _ in range(max_seq_length - token_len)]\n",
    "    return res_tokens\n",
    "\n",
    "def collate_batch(batch):\n",
    "    # tokenize both context and completion respectively\n",
    "    # (context and completion is delimited by \"\\n\")\n",
    "    context_list = list(zip(*batch))[0]\n",
    "    context_list = [c + \"\\n\" for c in context_list]\n",
    "    completion_list = list(zip(*batch))[1]\n",
    "    context_result = tokenizer(context_list)\n",
    "    context_tokens = context_result[\"input_ids\"]\n",
    "    context_masks = context_result[\"attention_mask\"]\n",
    "    completion_result = tokenizer(completion_list)\n",
    "    completion_tokens = completion_result[\"input_ids\"]\n",
    "    completion_masks = completion_result[\"attention_mask\"]\n",
    "    # concatenate token\n",
    "    inputs = [i + j for i, j in zip(context_tokens, completion_tokens)]\n",
    "    masks = [i + j for i, j in zip(context_masks, completion_masks)]\n",
    "    # create label\n",
    "    eos_id = tokenizer.encode(tokenizer.eos_token)[0]\n",
    "    labels = [t[1:] + [eos_id] for t in inputs]\n",
    "    labels = list(map(fill_ignore_label, labels, context_tokens))\n",
    "    # truncate and pad tokens\n",
    "    inputs = [pad_tokens(t, block_size, 0) for t in inputs] # OPT and GPT-2 doesn't use pad token (instead attn mask is used)\n",
    "    masks = [pad_tokens(t, block_size, 0) for t in masks]\n",
    "    labels = [pad_tokens(t, block_size, -100) for t in labels]\n",
    "    # convert to tensor\n",
    "    inputs = torch.tensor(inputs, dtype=torch.int64).to(device)\n",
    "    masks = torch.tensor(masks, dtype=torch.int64).to(device)\n",
    "    labels = torch.tensor(labels, dtype=torch.int64).to(device)\n",
    "    return inputs, labels, masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7267686f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "gradient_accumulation_steps = 16\n",
    "\n",
    "data = pd.read_json(\"train_formatted.jsonl\", lines=True)\n",
    "dataloader = DataLoader(\n",
    "    list(zip(data[\"context\"], data[\"completion\"])),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_batch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "53fe4bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoConfig\n",
    "\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    config=config,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb017aa",
   "metadata": {},
   "source": [
    "### before fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0a7533b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, input, mask, eos_id, pred_sequence_length):\n",
    "    predicted_last_id = -1\n",
    "    start_token_len = torch.sum(mask).cpu().numpy()\n",
    "    token_len = start_token_len\n",
    "    with torch.no_grad():\n",
    "        while (predicted_last_id != eos_id) and \\\n",
    "              (token_len - start_token_len < pred_sequence_length):\n",
    "            output = model(\n",
    "                input_ids=input,\n",
    "                attention_mask=mask,\n",
    "            )\n",
    "            predicted_ids = torch.argmax(output.logits, axis=-1).cpu().numpy()\n",
    "            predicted_last_id = predicted_ids[0][token_len - 1]\n",
    "            input[0][token_len] = predicted_last_id\n",
    "            mask[0][token_len] = 1\n",
    "            token_len = torch.sum(mask).cpu().numpy()\n",
    "    return input, token_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2b984820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, it is not a first, it is\n",
      "My name is Clara and I am name is Clara and I am name is Clara\n"
     ]
    }
   ],
   "source": [
    "eos_id = tokenizer.encode(tokenizer.eos_token)[0]\n",
    "\n",
    "result = tokenizer(\"Once upon a time,\")\n",
    "input = result[\"input_ids\"]\n",
    "mask = result[\"attention_mask\"]\n",
    "input = pad_tokens(input, block_size, 0)\n",
    "mask = pad_tokens(mask, block_size, 0)\n",
    "input = torch.tensor([input], dtype=torch.int64).to(device)\n",
    "mask = torch.tensor([mask], dtype=torch.int64).to(device)\n",
    "\n",
    "result_token, result_len = generate_text(\n",
    "    model,\n",
    "    input,\n",
    "    mask,\n",
    "    eos_id,\n",
    "    pred_sequence_length=15)\n",
    "print(tokenizer.decode(result_token[0][:result_len]))\n",
    "\n",
    "result = tokenizer(\"My name is Clara and I am\")\n",
    "input = result[\"input_ids\"]\n",
    "mask = result[\"attention_mask\"]\n",
    "input = pad_tokens(input, block_size, 0)\n",
    "mask = pad_tokens(mask, block_size, 0)\n",
    "input = torch.tensor([input], dtype=torch.int64).to(device)\n",
    "mask = torch.tensor([mask], dtype=torch.int64).to(device)\n",
    "\n",
    "result_token, result_len = generate_text(\n",
    "    model,\n",
    "    input,\n",
    "    mask,\n",
    "    eos_id,\n",
    "    pred_sequence_length=15)\n",
    "print(tokenizer.decode(result_token[0][:result_len]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906fede1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
